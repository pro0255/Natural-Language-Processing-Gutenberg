{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4c0717a-40cb-4d2f-ad7a-e00d47a4dcb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import os\n",
    "import json\n",
    "from enum import Enum\n",
    "from nltk import tokenize\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24183db6-e12a-4180-8127-4415016f9f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_CSV_DELIMITER = ';'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4a9396f-45ff-42bc-b17a-64e416c9662c",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTHOR_DIRECTORY_NAME = \"Authors\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c8ea4d8-8651-43b1-b63a-b7b609dbae86",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GutenbergJsonAttributes(Enum):\n",
    "    Title = \"Title\"\n",
    "    Author = \"Author\"\n",
    "    Subject = \"Subject\"\n",
    "    Alias = \"Alias\"\n",
    "    Birthdate = \"Birthdate\"\n",
    "    Deathdate = \"Deathdate\"\n",
    "    Aliases = \"Aliases\"\n",
    "    Text = \"Text\"\n",
    "    Id = \"Id\"\n",
    "    AuthorId = \"AuthorId\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5539b37c-7726-4e9d-82b3-e9c93cb038a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = \"*.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a968c73-3d10-42ba-b725-1b887dbaa32c",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_path = 'C:\\\\Users\\\\Vojta\\\\Desktop\\\\diploma\\\\gutenberg_json\\\\' + pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dbab0b09-b1cb-4dee-a50e-af64352ee5e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_files(path):\n",
    "    return tf.data.Dataset.list_files(path, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9e7ee9de-c73f-4073-adc2-258bbbc1dc19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json(path):\n",
    "    with open(path) as f:\n",
    "        return json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bf487e0b-7f2d-4a72-b726-168022450e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate_over_gutenberg(files_path, process_func):\n",
    "    print(f\"Loading files from {files_path}\")\n",
    "    for path_to_file in tqdm(load_files(files_path)):\n",
    "        path = bytes.decode(path_to_file.numpy())\n",
    "        data = load_json(path)\n",
    "        process_func(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a86302d7-1ee9-458f-9871-710e5551a3ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_SAVE_GENERAL_GUTENBERG = \"C:\\\\Users\\\\Vojta\\\\Desktop\\\\diploma\\\\gutenberg_from_raw\"\n",
    "PATH_TO_SAVE_DATASET = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9e6b33f4-0767-4190-9fb9-8c7ba4b6538f",
   "metadata": {},
   "outputs": [],
   "source": [
    "authors = pd.read_csv(PATH_TO_ALL_AUTHORS, sep=AUTHORS_CSV_DELIMITER, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "234f6228-7074-4fa3-822e-357e65e2d3e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gutenberg_author_id</th>\n",
       "      <th>author.y</th>\n",
       "      <th>n</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>761</td>\n",
       "      <td>Lytton, Edward Bulwer Lytton, Baron</td>\n",
       "      <td>215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1800</td>\n",
       "      <td>Ebers, Georg</td>\n",
       "      <td>164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>53</td>\n",
       "      <td>Twain, Mark</td>\n",
       "      <td>144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8659</td>\n",
       "      <td>Kingston, William Henry Giles</td>\n",
       "      <td>132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1285</td>\n",
       "      <td>Parker, Gilbert</td>\n",
       "      <td>131</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   gutenberg_author_id                             author.y    n\n",
       "0                  761  Lytton, Edward Bulwer Lytton, Baron  215\n",
       "1                 1800                         Ebers, Georg  164\n",
       "2                   53                          Twain, Mark  144\n",
       "3                 8659        Kingston, William Henry Giles  132\n",
       "4                 1285                      Parker, Gilbert  131"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "authors.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b21112ef-aaa3-483d-93bc-29f70b63c7fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data_as_raw_row(data):\n",
    "    current_id = data[GutenbergJsonAttributes.Id.value]\n",
    "    copied = data.copy()\n",
    "    \n",
    "    #TODO: Maybe some kind of general processing\n",
    "    processed_text = copied[GutenbergJsonAttributes.Text.value][0]\n",
    "\n",
    "    copied[GutenbergJsonAttributes.Text.value] = [processed_text]\n",
    "    \n",
    "    path = PATH_TO_SAVE_GENERAL_GUTENBERG + \"\\\\\" + str(current_id[0]) + \".json\"\n",
    "    \n",
    "    with open(path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(copied, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "90a10c67-a9d9-4360-bd3d-a68232b98b29",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'process_data_as_raw_row' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [22]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[0m iterate_over_gutenberg(raw_path, \u001b[43mprocess_data_as_raw_row\u001b[49m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'process_data_as_raw_row' is not defined"
     ]
    }
   ],
   "source": [
    "iterate_over_gutenberg(raw_path, process_data_as_raw_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5961d629-fd6e-4a36-877e-8858f73ea7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_newlines(document):\n",
    "    return document.replace('\\n', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "75937bf5-1a9e-42d6-a158-31e39a5c8930",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_project_delimiter(document):\n",
    "    return document.replace(PROJECT_CSV_DELIMITER, '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c40905bc-7c59-4532-8d92-c79f169023a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_document_by_sentence(document, k):\n",
    "    preprocessed_document_for_sentences = preprocess_newlines(document)\n",
    "    preprocessed_document_for_sentences = preprocess_project_delimiter(preprocessed_document_for_sentences)\n",
    "    sentences = tokenize.sent_tokenize(preprocessed_document_for_sentences)\n",
    "    chunked_sentences = [' '.join(sentences[sent_index:sent_index+k]) for sent_index in range(0, len(sentences), k)]\n",
    "    return chunked_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4627f402-873e-4de9-9430-dca27e3067b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_process_func(k, name, path, authors_tuple):\n",
    "    number_of_authors = len(authors_tuple)\n",
    "    authors_ids, authors_names = zip(*authors_tuple)\n",
    "    \n",
    "    directory_for_file = os.sep.join([path, f'{number_of_authors}Authors', name])\n",
    "    \n",
    "    if not os.path.exists(directory_for_file):\n",
    "        os.makedirs(directory_for_file)\n",
    "    \n",
    "    authors_dataframe = pd.DataFrame.from_dict({\n",
    "        GutenbergJsonAttributes.AuthorId.value: authors_ids, \n",
    "        GutenbergJsonAttributes.Author.value: authors_names\n",
    "    },)\n",
    "    \n",
    "    \n",
    "    name_of_authors_file = \"authors.csv\"\n",
    "    authors_save = os.sep.join([directory_for_file, name_of_authors_file])\n",
    "                               \n",
    "    print(f'Saving authors csv to {authors_save}')\n",
    "                               \n",
    "    authors_dataframe.to_csv(authors_save, index=False, sep=';')\n",
    "    \n",
    "    def process_to_create_dataset(data):\n",
    "        current_author = data[GutenbergJsonAttributes.Author.value][0] \n",
    "        current_text = data[GutenbergJsonAttributes.Text.value][0]\n",
    "        current_author_id = data[GutenbergJsonAttributes.AuthorId.value][0] \n",
    "        \n",
    "        is_required_author = current_author_id in authors_ids\n",
    "        \n",
    "        if is_required_author:\n",
    "            \n",
    "            chunked_sentences = chunk_document_by_sentence(current_text, k)\n",
    "\n",
    "            name_of_file = \"data.csv\"\n",
    "            with open(os.sep.join([directory_for_file, name_of_file]), 'a', newline='') as f:\n",
    "                writer = csv.writer(f, delimiter=';')\n",
    "\n",
    "                for chunk in chunked_sentences:\n",
    "                    value = [chunk, current_author_id]\n",
    "                    writer.writerow(value)\n",
    "\n",
    "            return chunked_sentences\n",
    "        \n",
    "        return []\n",
    "\n",
    "    return process_to_create_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ef7dbaeb-7ab1-4001-9f7f-86d20444edac",
   "metadata": {},
   "outputs": [],
   "source": [
    "DIRECTORY_TO_SAVE = \"C:\\\\Users\\\\Vojta\\\\Desktop\\\\diploma\\\\data\\\\gutenberg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2c8c144f-6630-497e-b887-0798fa921ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_ALL_AUTHORS = \"C:\\\\Users\\\\Vojta\\\\Desktop\\\\diploma\\\\gutenberg_downloaded\\\\authors\\\\authors.csv\"\n",
    "AUTHORS_CSV_DELIMITER = \",\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "685401d3-4c9f-4d88-900d-fd38100257b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AuthorsGenerator:\n",
    "    def __init__(self, path: str, sep: str):\n",
    "        self.path = path\n",
    "        self.sep = sep\n",
    "    \n",
    "    def generate_top_k(self, k: int):\n",
    "        data = pd.read_csv(self.path, sep=self.sep)\n",
    "        rows = data.shape[0]\n",
    "        authors_ids = data.iloc[0:rows-1, 0].astype(int).values[0:k]\n",
    "        authors_names = data.iloc[0:rows-1, 1].astype(str).values[0:k]\n",
    "        authors_tuple = list(zip(authors_ids, authors_names))\n",
    "        del data\n",
    "        return authors_tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "73e86e38-036e-444b-a893-92411f2288b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "authors_generator = AuthorsGenerator(PATH_TO_ALL_AUTHORS, AUTHORS_CSV_DELIMITER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "544f6e2b-7d30-450e-84d8-eba99b7953f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_SAVE_TEST = \"C:\\\\Users\\\\Vojta\\\\Desktop\\\\diploma\\\\gutenberg_test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "abde441f-e8a6-4f2b-97a0-f21375d37922",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_path = os.sep.join([PATH_TO_SAVE_TEST, pattern])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e71dbfbe-b86b-4c74-9446-aa2424a51363",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving authors csv to C:\\Users\\Vojta\\Desktop\\diploma\\data\\gutenberg\\10Authors\\Sentence3\\authors.csv\n",
      "Loading files from C:\\Users\\Vojta\\Desktop\\diploma\\gutenberg_test\\*.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 90.93it/s]\n"
     ]
    }
   ],
   "source": [
    "iterate_over_gutenberg(test_path, build_process_func(3, \"Sentence\" + str(3), DIRECTORY_TO_SAVE, authors_generator.generate_top_k(10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1993dea1-57ff-4dd0-96c6-116550725cb6",
   "metadata": {},
   "source": [
    "# Have big csv ... iterate over csv and create test train valid sets deps on statistcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "405b45ae-fc7f-4170-98a2-746a2a3d3f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_DATASET_FOLDER = \"C:\\\\Users\\\\Vojta\\\\Desktop\\\\diploma\\\\data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c1219cf7-3a87-4f69-865f-9ae6076e5445",
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "import os.path\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2151576a-3e3c-4489-a777-505f41d12ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataSet(Enum):\n",
    "    Gutenberg = \"gutenberg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8b7ffc7b-ba90-4734-9b2a-adba5041a3c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataSetType(Enum):\n",
    "    Sentence = \"Sentence\"\n",
    "    Article = \"Article\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "16fecc26-4da0-4e7f-a444-3e1fef756ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_NAME = 'data.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "1d3f4ee9-8bf3-47f3-b947-d2d15318113f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_path(directory, dataset, authors_directory, dataset_type, k=None):\n",
    "    is_sentence_type = dataset_type == DataSetType.Sentence \n",
    "    if is_sentence_type and k is None:\n",
    "        raise Exception(f\"Sentence should be specified with k argument!\")\n",
    "    \n",
    "    return os.path.join(directory, dataset.value, authors_directory, dataset_type.value + str(k), DATA_NAME) if is_sentence_type else os.path.join(directory, dataset.value, authors_directory, dataset_type.value, DATA_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "eb6485bf-73a1-4d35-86d7-e73df5d71cb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Vojta\\\\Desktop\\\\diploma\\\\data\\\\gutenberg\\\\10Authors\\\\Sentence10\\\\data.csv'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_path(PATH_TO_DATASET_FOLDER, DataSet.Gutenberg, create_author_directory(10), DataSetType.Sentence, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "de786555-9277-4fdd-87d7-1df3fffb8954",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(text):\n",
    "    return text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "737c8bd5-23e2-4686-b774-6e2fcdb46a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset_object_from_path(csv_filename, delim, text_pipeline_func=None):\n",
    "    dataset = tf.data.TextLineDataset(filenames=csv_filename)\n",
    "    \n",
    "    def parse_csv(line):\n",
    "        csv_line = bytes.decode(line.numpy())\n",
    "        text, author = csv_line.split(delim)\n",
    "        if text_pipeline_func is not None:\n",
    "            text = text_pipeline_func(text)\n",
    "        return text, author \n",
    "\n",
    "    dataset = dataset.map(lambda tpl: tf.py_function(parse_csv, [tpl], [tf.string, tf.string]))\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "767e856e-66bf-469a-a8ff-76ffce758b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = get_dataset_object_from_path(create_path(PATH_TO_DATASET_FOLDER, DataSet.Gutenberg, DataSetType.Sentence, 3), ';', process_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f60752-fb3b-4526-86de-4c642d0275ed",
   "metadata": {},
   "source": [
    "# Create statistics for specified dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "013ccc9c-33d1-4a16-bdd5-b319b90a26db",
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME_OF_STATISTICS_FILE = 'stats_'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "7cb3fa04-24cb-4a0a-b572-94cfbe72c7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_stats_file_name(name_of_file):\n",
    "    return f'{NAME_OF_STATISTICS_FILE}{name_of_file}.xlsx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "a5a53918-742c-445a-a066-5759d5123770",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_input_for_statistics(path, sep, metric_instances = [] ,text_pipeline_func = None, save = True):\n",
    "    path_parts = path.split(os.path.sep)\n",
    "    name_of_file = path_parts[-1]\n",
    "    del path_parts[-1]\n",
    "    path_parts.append(create_stats_file_name(name_of_file))\n",
    "    path_to_save = os.path.sep.join(path_parts)\n",
    "    return get_dataset_object_from_path(path, sep, text_pipeline_func), MetricWrapper(metric_instances, path_to_save if save else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "edfbcc26-550b-4ffc-bdce-1f4f70f8932b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#build_input_for_statistics(create_path(PATH_TO_DATASET_FOLDER, DataSet.Gutenberg, DataSetType.Sentence, 3), ';', process_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89886737-48b2-4837-a8ac-e87b935c3937",
   "metadata": {},
   "source": [
    "### TODO: create more metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "45da1048-9c5d-4165-99d9-c4d10f50fcbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_author_directory(k):\n",
    "    return f'{k}{AUTHOR_DIRECTORY_NAME}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "25375336-7565-4589-9b74-69a20ff6cd9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelMetric:\n",
    "    def __init__(self):\n",
    "        self.state = {}\n",
    "    \n",
    "    def update_state(self, text, label):\n",
    "        self.state[label] = self.state.get(label, 0) + 1\n",
    "        \n",
    "    def get_dataframe(self):\n",
    "        return pd.DataFrame.from_dict(self.state, orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "49f6c56a-8fe0-47f2-87d1-f02d56ddb2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetricWrapper:\n",
    "    def __init__(self, metric_instances = [], path_to_save = None):\n",
    "        self.path_to_save = path_to_save\n",
    "        self.metric_instances = metric_instances\n",
    "    \n",
    "    def update_state(self, text, label):\n",
    "        #TODO: Can derive from this class and update __init__ and update_states\n",
    "        for instance in self.metric_instances:\n",
    "            instance.update_state(text, label)\n",
    "    \n",
    "    def process_row(self, record):\n",
    "        text, label = record\n",
    "        text = bytes.decode(text.numpy())\n",
    "        label = bytes.decode(label.numpy())\n",
    "        self.update_state(text, label)\n",
    "    \n",
    "    def save(self):\n",
    "        if self.path_to_save is not None:\n",
    "            print(f'Saving to {self.path_to_save}')\n",
    "            with pd.ExcelWriter(self.path_to_save, engine='xlsxwriter') as writer:\n",
    "                for metric_instance in self.metric_instances:\n",
    "                    metric_instance.get_dataframe().to_excel(writer, sheet_name=type(metric_instance).__name__)            \n",
    "        else:\n",
    "            print('Saving path is not specified!')\n",
    "            \n",
    "    def __str__(self):\n",
    "        string = ''\n",
    "        for instance in self.metric_instances:\n",
    "            string += type(instance).__name__\n",
    "            string += instance.get_dataframe().to_string()\n",
    "            string += '\\n'\n",
    "        return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "24ab76f9-4ea0-4c1b-9236-895c0d1e9edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_statistics_from(dataset, metric_instance):\n",
    "    for i, record in enumerate(dataset):\n",
    "        metric_instance.process_row(record)\n",
    "    metric_instance.save()\n",
    "    return metric_instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "1be95d26-cd97-421c-9e0b-094b7e9c8c7e",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'value'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [123]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m metric_instance \u001b[38;5;241m=\u001b[39m create_statistics_from(\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;241m*\u001b[39mbuild_input_for_statistics(\n\u001b[1;32m----> 3\u001b[0m         \u001b[43mcreate_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43mPATH_TO_DATASET_FOLDER\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDataSet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mGutenberg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDataSetType\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSentence\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m, \n\u001b[0;32m      4\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m;\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[0;32m      5\u001b[0m         [LabelMetric()], \n\u001b[0;32m      6\u001b[0m         process_text,\n\u001b[0;32m      7\u001b[0m         \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m      8\u001b[0m     )\n\u001b[0;32m      9\u001b[0m )\n",
      "Input \u001b[1;32mIn [70]\u001b[0m, in \u001b[0;36mcreate_path\u001b[1;34m(directory, dataset, authors_directory, dataset_type, k)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_sentence_type \u001b[38;5;129;01mand\u001b[39;00m k \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSentence should be specified with k argument!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(directory, dataset\u001b[38;5;241m.\u001b[39mvalue, authors_directory, dataset_type\u001b[38;5;241m.\u001b[39mvalue \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(k), DATA_NAME) \u001b[38;5;28;01mif\u001b[39;00m is_sentence_type \u001b[38;5;28;01melse\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(directory, dataset\u001b[38;5;241m.\u001b[39mvalue, authors_directory, \u001b[43mdataset_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m, DATA_NAME)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'int' object has no attribute 'value'"
     ]
    }
   ],
   "source": [
    "metric_instance = create_statistics_from(\n",
    "    *build_input_for_statistics(\n",
    "        create_path(PATH_TO_DATASET_FOLDER, DataSet.Gutenberg, DataSetType.Sentence, 3), \n",
    "        ';', \n",
    "        [LabelMetric()], \n",
    "        process_text,\n",
    "        True\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "654cbf9c-e401-4c38-88ad-42324d0e5fe4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'metric_instance' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [124]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mmetric_instance\u001b[49m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'metric_instance' is not defined"
     ]
    }
   ],
   "source": [
    "print(metric_instance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "04199aae-4f66-4e95-aa27-9f3ca2eaf09d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_file_to_train_test_valid(path_to_file, train_size=1, test_size=0, valid_size=0, min_label_size=None):\n",
    "    print(train_size, test_size, valid_size, path_to_file, min_label_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "31a143c3-af25-409b-949d-3fdd22ba9270",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0 0 C:\\Users\\Vojta\\Desktop\\diploma\\data\\gutenberg\\10Authors\\Sentence3\\data.csv None\n"
     ]
    }
   ],
   "source": [
    "split_file_to_train_test_valid(\n",
    "    create_path(PATH_TO_DATASET_FOLDER, DataSet.Gutenberg, create_author_directory(10), DataSetType.Sentence, 3),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25f1ed8d-253c-41c3-a7ba-86a7f661b4e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_DATASET_FOLDER = \"C:\\\\Users\\\\Vojta\\\\Desktop\\\\diploma\\\\data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b459283-908b-4205-b32d-ce39fb233b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "import os.path\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c47e7d7c-8392-4996-86ca-f45fd4b8dcc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataSet(Enum):\n",
    "    Gutenberg = \"gutenberg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f1fc5e4-2f56-4815-bee2-a1251cdfd901",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataSetType(Enum):\n",
    "    Sentence = \"Sentence\"\n",
    "    Article = \"Article\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "964d4900-5ce7-411a-ba8b-37f9cd76c55b",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_NAME = 'data.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e763d64-ae86-4e57-b0e8-039c11ac7888",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_path(directory, dataset, dataset_type, k=None):\n",
    "    is_sentence_type = dataset_type == DataSetType.Sentence \n",
    "    if is_sentence_type and k is None:\n",
    "        raise Exception(f\"Sentence should be specified with k argument!\")\n",
    "    \n",
    "    return os.path.join(directory, dataset.value, dataset_type.value + str(k), DATA_NAME) if is_sentence_type else os.path.join(directory, dataset.value, dataset_type.value, DATA_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a0014248-0698-4519-8999-23b332d73f2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Vojta\\\\Desktop\\\\diploma\\\\data\\\\gutenberg\\\\Sentence10\\\\data.csv'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_path(PATH_TO_DATASET_FOLDER, DataSet.Gutenberg, DataSetType.Sentence, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "036fc977-20e7-46d5-901d-bda3dbb02f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(text):\n",
    "    return text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "97168c0a-fb11-4660-94c3-70d82041e4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset_object_from_path(csv_filename, delim, text_pipeline_func=None):\n",
    "    dataset = tf.data.TextLineDataset(filenames=csv_filename)\n",
    "    \n",
    "    def parse_csv(line):\n",
    "        csv_line = bytes.decode(line.numpy())\n",
    "        text, author = csv_line.split(delim)\n",
    "        if text_pipeline_func is not None:\n",
    "            text = text_pipeline_func(text)\n",
    "        return text, author \n",
    "\n",
    "    dataset = dataset.map(lambda tpl: tf.py_function(parse_csv, [tpl], [tf.string, tf.string]))\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2b33a960-cdbd-4a76-afc5-bbbdf0b45f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = get_dataset_object_from_path(create_path(PATH_TO_DATASET_FOLDER, DataSet.Gutenberg, DataSetType.Sentence, 3), ';', process_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b28e47e-0fac-4a55-a244-396cae8bef17",
   "metadata": {},
   "source": [
    "# Create statistics for specified dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "328c31ea-f305-4133-8bda-840e57060384",
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME_OF_STATISTICS_FILE = 'stats_'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ea2c5801-fbf7-4cf1-8acc-56f8ab2c2adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_stats_file_name(name_of_file):\n",
    "    return f'{NAME_OF_STATISTICS_FILE}{name_of_file}.xlsx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ef62eab0-cdaa-4a5f-9128-b7ac5667e255",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_input_for_statistics(path, sep, metric_instances = [] ,text_pipeline_func = None, save = True):\n",
    "    path_parts = path.split(os.path.sep)\n",
    "    name_of_file = path_parts[-1]\n",
    "    del path_parts[-1]\n",
    "    path_parts.append(create_stats_file_name(name_of_file))\n",
    "    path_to_save = os.path.sep.join(path_parts)\n",
    "    return get_dataset_object_from_path(path, sep, text_pipeline_func), MetricWrapper(metric_instances, path_to_save if save else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "19a9b56c-12f2-40b6-905c-41c34cf22210",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<MapDataset shapes: (<unknown>, <unknown>), types: (tf.string, tf.string)>,\n",
       " <__main__.MetricWrapper at 0x22005ca2430>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "build_input_for_statistics(create_path(PATH_TO_DATASET_FOLDER, DataSet.Gutenberg, DataSetType.Sentence, 3), ';', process_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff5f9c0-efec-4a0e-a9a3-51e38b178f89",
   "metadata": {},
   "source": [
    "### TODO: create more metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aca3c854-a4fa-44f7-9d6a-33d84ab77a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelMetric:\n",
    "    def __init__(self):\n",
    "        self.state = {}\n",
    "    \n",
    "    def update_state(self, text, label):\n",
    "        self.state[label] = self.state.get(label, 0) + 1\n",
    "        \n",
    "    def get_dataframe(self):\n",
    "        return pd.DataFrame.from_dict(self.state, orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "436a8861-bed2-495c-812f-cd9e90aadb6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetricWrapper:\n",
    "    def __init__(self, metric_instances = [], path_to_save = None):\n",
    "        self.path_to_save = path_to_save\n",
    "        self.metric_instances = metric_instances\n",
    "    \n",
    "    def update_state(self, text, label):\n",
    "        #TODO: Can derive from this class and update __init__ and update_states\n",
    "        for instance in self.metric_instances:\n",
    "            instance.update_state(text, label)\n",
    "    \n",
    "    def process_row(self, record):\n",
    "        text, label = record\n",
    "        text = bytes.decode(text.numpy())\n",
    "        label = bytes.decode(label.numpy())\n",
    "        self.update_state(text, label)\n",
    "    \n",
    "    def save(self):\n",
    "        if self.path_to_save is not None:\n",
    "            print(f'Saving to {self.path_to_save}')\n",
    "            with pd.ExcelWriter(self.path_to_save, engine='xlsxwriter') as writer:\n",
    "                for metric_instance in self.metric_instances:\n",
    "                    metric_instance.get_dataframe().to_excel(writer, sheet_name=type(metric_instance).__name__)            \n",
    "        else:\n",
    "            print('Saving path is not specified!')\n",
    "            \n",
    "    def __str__(self):\n",
    "        string = ''\n",
    "        for instance in self.metric_instances:\n",
    "            string += type(instance).__name__\n",
    "            string += instance.get_dataframe().to_string()\n",
    "            string += '\\n'\n",
    "        return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "22e3b7bc-6993-4536-9588-b9786c13bf0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_statistics_from(dataset, metric_instance):\n",
    "    for i, record in enumerate(dataset):\n",
    "        metric_instance.process_row(record)\n",
    "    metric_instance.save()\n",
    "    return metric_instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "079c3552-746f-4b61-b535-fa388acfc2e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving to C:\\Users\\Vojta\\Desktop\\diploma\\data\\gutenberg\\Sentence3\\stats_data.csv.xlsx\n"
     ]
    }
   ],
   "source": [
    "metric_instance = create_statistics_from(\n",
    "    *build_input_for_statistics(\n",
    "        create_path(PATH_TO_DATASET_FOLDER, DataSet.Gutenberg, DataSetType.Sentence, 3), \n",
    "        ';', \n",
    "        [LabelMetric()], \n",
    "        process_text,\n",
    "        True\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3c667fcb-a90a-499f-a500-5567b8203367",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LabelMetric                                    0\n",
      "Twain, Mark                      3933\n",
      "Meredith, George                  374\n",
      "Jacobs, W. W. (William Wymark)  18356\n",
      "Fenn, George Manville             302\n",
      "Balzac, Honoré de                 495\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metric_instance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "52528560-7ac5-4792-9dea-bcedacb647fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_SIZE = 0.7\n",
    "VALIDATION_SIZE = 0.15\n",
    "TEST_SIZE = 0.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "15bfe7eb-2f91-4c58-9ce7-8e4301bf96db",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_NAME = 'train.csv'\n",
    "TEST_NAME = 'test.csv'\n",
    "VALIDATION_NAME = 'valid.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "fee99e20-dacf-469b-a77c-b78699536a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_size(train_size=1, test_size=0, valid_size=0):\n",
    "    if np.sum([train_size, test_size, valid_size]) != 1:\n",
    "        assert Exception(\"Is not valid split!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "id": "00a82ba6-74ad-4d6c-a975-45994c70b897",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_from(path):\n",
    "    if os.path.exists(path):\n",
    "        os.remove(path)\n",
    "        print(f'Deleting file {path}')\n",
    "    else:\n",
    "        print(f'File {path} do not exists')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "id": "ea90baa7-9fd3-48f9-8538-856f927e064e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataSetSplitter:\n",
    "    def __init__(self, path_to_save, train_size, test_size, valid_size, label_counter, normalization_size):\n",
    "        self.directory_path = path_to_save\n",
    "        self.size_file_name = 'subset_sizes.csv'\n",
    "        self.create_state(train_size, test_size, valid_size)\n",
    "        self.delete_files_in_directory()\n",
    "        self.counter_dataframe = label_counter.copy()\n",
    "        self.normalize(normalization_size)  \n",
    "        self.prepare_counters()\n",
    "        self.save_counters()\n",
    "\n",
    "        \n",
    "    def create_state(self, train_size, test_size, valid_size):\n",
    "        self.state = {\n",
    "            'train': {\n",
    "                'path': os.path.sep.join([self.directory_path, TRAIN_NAME]),\n",
    "                'size': train_size\n",
    "            \n",
    "            },\n",
    "            'test': {\n",
    "                'path': os.path.sep.join([self.directory_path, TEST_NAME]),\n",
    "                'size': test_size\n",
    "            \n",
    "            },\n",
    "            'valid': {\n",
    "                'path': os.path.sep.join([self.directory_path, VALIDATION_NAME]),\n",
    "                'size': valid_size\n",
    "            \n",
    "            },\n",
    "        }\n",
    "        \n",
    "    def normalize(self, normalization_size):\n",
    "        if normalization_size is not None:\n",
    "            self.counter_dataframe.iloc[:, 0] =  normalization_size\n",
    "               \n",
    "    def prepare_counters(self):\n",
    "        dataset_counters = {name:{} for name in self.state.keys()}\n",
    "    \n",
    "        for key in dataset_counters.keys():\n",
    "            set_key_counter = {}\n",
    "            for author_id, row in self.counter_dataframe.iterrows():\n",
    "                count = row[0]\n",
    "                set_key_counter[author_id] = math.floor(count * self.state[key]['size'])\n",
    "            dataset_counters[key] = set_key_counter\n",
    "    \n",
    "        \n",
    "        self.dataset_counters = dataset_counters\n",
    "        \n",
    "    def get_path(self, label):\n",
    "        picks = []\n",
    "    \n",
    "        for key in self.dataset_counters.keys():\n",
    "            if self.dataset_counters[key][label] > 0:\n",
    "                picks.append(key)\n",
    "        \n",
    "        if len(picks) == 0:\n",
    "            return None\n",
    "        #choice one\n",
    "        pick = random.choice(picks)\n",
    "        #subtract\n",
    "        self.dataset_counters[pick][label] -= 1\n",
    "        #return path according to pick\n",
    "        return self.state[pick]['path']\n",
    "    \n",
    "    def save_counters(self):\n",
    "        counters = pd.DataFrame.from_dict(self.dataset_counters, orient='index')\n",
    "        counters.to_csv(os.path.sep.join([self.directory_path, self.size_file_name]), sep=';')\n",
    "        \n",
    "        \n",
    "    def delete_files_in_directory(self):\n",
    "        for v in self.state.values():\n",
    "            current_subset_path = v['path']\n",
    "            delete_from(current_subset_path)\n",
    "        delete_from(os.path.sep.join([self.directory_path, self.size_file_name]))\n",
    "                \n",
    "                \n",
    "    def build_subsets(self, dataset):\n",
    "        for line in tqdm(dataset.shuffle(10000).as_numpy_iterator()):\n",
    "            #label = author Id\n",
    "            text, label = line\n",
    "            text = bytes.decode(text)\n",
    "            \n",
    "            #TODO: DELETE!\n",
    "            label = bytes.decode(label)\n",
    "            \n",
    "            path = self.get_path(label)\n",
    "            \n",
    "            if path is None:\n",
    "                continue\n",
    "                \n",
    "            with open(path, 'a', newline='') as f:\n",
    "                writer = csv.writer(f, delimiter=';')\n",
    "                value = [text, label]\n",
    "                writer.writerow(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "id": "e56670b9-5436-4c00-a2f7-0da2284422b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_file_to_train_test_valid(\n",
    "    path_to_load, \n",
    "    path_to_save, \n",
    "    label_metric=None,\n",
    "    normalization_size=None,\n",
    "    train_size=TRAIN_SIZE, \n",
    "    test_size=TEST_SIZE, \n",
    "    valid_size=VALIDATION_SIZE,\n",
    "):\n",
    "    check_size(train_size, test_size, valid_size)\n",
    "    splitter = DataSetSplitter(path_to_save, train_size, test_size, valid_size, label_metric, normalization_size)\n",
    "    \n",
    "    dataset = get_dataset_object_from_path(path_to_load, ';', None)\n",
    "    splitter.build_subsets(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "id": "05c4e63c-392b-4886-9a22-507f7cc43bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_split_deps_on_stats(path_to_load, path_to_save, normalization = True, train_size=TRAIN_SIZE, test_size=TEST_SIZE, valid_size=VALIDATION_SIZE):\n",
    "    check_size(train_size, test_size, valid_size)\n",
    "    \n",
    "    metric_instance = create_statistics_from(\n",
    "        *build_input_for_statistics(\n",
    "            path_to_load,\n",
    "            ';', \n",
    "            [LabelMetric()], \n",
    "            process_text,\n",
    "            False\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    number_of_min_label = None\n",
    "    label_metric = metric_instance.metric_instances[0].get_dataframe()\n",
    "    \n",
    "    if normalization:\n",
    "        sorted_label_metric_frame = label_metric.sort_values(by=0)\n",
    "        id_of_min_label = sorted_label_metric_frame.iloc[0].name\n",
    "        number_of_min_label = sorted_label_metric_frame.iloc[0][0]\n",
    "    \n",
    "    split_file_to_train_test_valid(path_to_load, path_to_save, label_metric, number_of_min_label)\n",
    "    return label_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "id": "72d07737-e34c-4fbf-9ea4-ce2d780b2207",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Vojta\\\\Desktop\\\\diploma\\\\data\\\\gutenberg\\\\10Authors\\\\Sentence3\\\\data.csv'"
      ]
     },
     "execution_count": 429,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = create_path(PATH_TO_DATASET_FOLDER, DataSet.Gutenberg, create_author_directory(10), DataSetType.Sentence, 3)\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "id": "ce08db41-36ba-422b-b689-1ecab61f37c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving path is not specified!\n",
      "Deleting file C:\\Users\\Vojta\\Desktop\\diploma\\data\\gutenberg\\10Authors\\Sentence3\\train.csv\n",
      "Deleting file C:\\Users\\Vojta\\Desktop\\diploma\\data\\gutenberg\\10Authors\\Sentence3\\test.csv\n",
      "Deleting file C:\\Users\\Vojta\\Desktop\\diploma\\data\\gutenberg\\10Authors\\Sentence3\\valid.csv\n",
      "Deleting file C:\\Users\\Vojta\\Desktop\\diploma\\data\\gutenberg\\10Authors\\Sentence3\\subset_sizes.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23460it [00:23, 999.02it/s] \n"
     ]
    }
   ],
   "source": [
    "res = run_split_deps_on_stats(\n",
    "    p,\n",
    "    os.path.sep.join(p.split(os.path.sep)[0:-1]),\n",
    "    False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49b9d79-b087-4e99-b5e8-2b650f47b504",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84769cc3-e9ce-4ca2-af4f-6df4923bddd3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

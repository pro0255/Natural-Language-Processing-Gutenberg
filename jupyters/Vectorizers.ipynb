{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "def adding_module_path():\n",
    "    module_path = os.path.abspath(os.path.join('..'))\n",
    "\n",
    "    if module_path not in sys.path:\n",
    "        sys.path.append(module_path)\n",
    "\n",
    "adding_module_path()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_corpus = np.array([\n",
    "    'This is the first document.',\n",
    "    'This document is the second document.',\n",
    "    'And this is the third one.',\n",
    "    'Is this the first document?',\n",
    "])\n",
    "y = np.array([0,0,0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset from=C:\\Users\\Vojta\\Desktop\\diploma\\data_test\\gutenberg\\10Authors\\Sentence3\\train.csv\n",
      "Loading dataset from=C:\\Users\\Vojta\\Desktop\\diploma\\data_test\\gutenberg\\10Authors\\Sentence3\\valid.csv\n",
      "Loading dataset from=C:\\Users\\Vojta\\Desktop\\diploma\\data_test\\gutenberg\\10Authors\\Sentence3\\test.csv\n"
     ]
    }
   ],
   "source": [
    "path_data, path_authors = get_path_to_gutenberg_sets(10, 3, PATH_TO_DATASET_FOLDER_TEST)\n",
    "train, valid, test = get_datasets(path_data, ';', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((test_corpus, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(), dtype=string, numpy=b'This is the first document.'>, <tf.Tensor: shape=(), dtype=int32, numpy=0>)\n"
     ]
    }
   ],
   "source": [
    "for x in dataset:\n",
    "    print(x)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BoW Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "BoWVectorizer = CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 1, 1, 0, 0, 1, 0, 1],\n",
       "       [0, 2, 0, 1, 0, 1, 1, 0, 1],\n",
       "       [1, 0, 0, 1, 1, 0, 1, 1, 1],\n",
       "       [0, 1, 1, 1, 0, 0, 1, 0, 1]], dtype=int64)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = BoWVectorizer()\n",
    "X = vectorizer.fit_transform(test_corpus)\n",
    "vectorizer.get_feature_names_out()\n",
    "X.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TFIDF Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "TFIDFVectorizer= TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.46979139, 0.58028582, 0.38408524, 0.        ,\n",
       "        0.        , 0.38408524, 0.        , 0.38408524],\n",
       "       [0.        , 0.6876236 , 0.        , 0.28108867, 0.        ,\n",
       "        0.53864762, 0.28108867, 0.        , 0.28108867],\n",
       "       [0.51184851, 0.        , 0.        , 0.26710379, 0.51184851,\n",
       "        0.        , 0.26710379, 0.51184851, 0.26710379],\n",
       "       [0.        , 0.46979139, 0.58028582, 0.38408524, 0.        ,\n",
       "        0.        , 0.38408524, 0.        , 0.38408524]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "vectorizer = TFIDFVectorizer()\n",
    "X = vectorizer.fit_transform(test_corpus)\n",
    "vectorizer.get_feature_names_out()\n",
    "X.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFAutoModel\n",
    "from src.types.transformer_name import TransformerName\n",
    "from src.tokenizers.transformer_tokenizer import TransformerTokenizer\n",
    "from src.encoder.create_encoder_from_path import create_encoder_from_path\n",
    "from src.tokenizers.prepare_dataset_from_tokenizer import prepare_dataset_from_tokenizer\n",
    "from src.types.transformer_pooling import TransformerPooling\n",
    "from src.models.transformer.bert_pooling_layer import BertPoolingLayer\n",
    "from transformers import AutoConfig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerVectorizer():\n",
    "    def __init__(\n",
    "        self, \n",
    "        transformer_type,\n",
    "        transformer_pooling_type,\n",
    "        path_authors=None,\n",
    "        encoder=None, \n",
    "        max_len=512, \n",
    "        preprocess_pipeline=None,\n",
    "    ):\n",
    "        self.transformer_type = transformer_type.value\n",
    "        self.transformer_pooling_type = transformer_pooling_type\n",
    "        self.encoder = encoder\n",
    "        self.max_len = max_len\n",
    "        self.preprocess_pipeline = preprocess_pipeline\n",
    "        self.path_to_authors = path_authors\n",
    "        self.setup()\n",
    "\n",
    "    def setup(self):\n",
    "        self.config = AutoConfig.from_pretrained(self.transformer_type, output_hidden_states=True)\n",
    "        self.transformer = TFAutoModel.from_config(self.config)\n",
    "        encoder = None if self.path_to_authors is None else create_encoder_from_path(self.path_to_authors)\n",
    "        self.tokenizer = TransformerTokenizer(\n",
    "            self.transformer_type, \n",
    "            encoder\n",
    "        )\n",
    "\n",
    "    def fit_transform(self, dataset):\n",
    "        #imagine X as list of sentences\n",
    "        sentence_embedding = []\n",
    "        labels = []\n",
    "\n",
    "        for x in prepare_dataset_from_tokenizer(dataset, self.tokenizer).batch(1):\n",
    "            transformer_input, label = x\n",
    "            output = self.transformer(\n",
    "                transformer_input, \n",
    "                output_hidden_states=True\n",
    "            )\n",
    "            \n",
    "            output = BertPoolingLayer()(\n",
    "                output, \n",
    "                self.transformer_pooling_type\n",
    "            )\n",
    "\n",
    "            output = output.numpy().reshape(-1)\n",
    "            label = label.numpy()[0] \n",
    "\n",
    "            labels.append(label)\n",
    "            sentence_embedding.append(output)\n",
    "            break\n",
    "        return np.array(sentence_embedding), np.array(labels)\n",
    "\n",
    "    def create_embedding_matrix(self, X):\n",
    "        #TODO: add to embedding layer\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertBaseUncasedVectorizer(TransformerVectorizer):\n",
    "    def __init__(\n",
    "        self, \n",
    "        transformer_pooling_type,\n",
    "        path_authors=None,\n",
    "        encoder=None, \n",
    "        max_len=512, \n",
    "        preprocess_pipeline=None,\n",
    "    ):\n",
    "        super().__init__(\n",
    "            TransformerName.BertBaseUncased,         \n",
    "            transformer_pooling_type,\n",
    "            path_authors=None,\n",
    "            encoder=None, \n",
    "            max_len=512, \n",
    "            preprocess_pipeline=None\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert = BertBaseUncasedVectorizer()\n",
    "X, y = bert.fit_transform(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 768)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DistilBertBaseUncasedVectorizer(TransformerVectorizer):\n",
    "    def __init__(\n",
    "        self, \n",
    "        transformer_pooling_type,\n",
    "        path_authors=None,\n",
    "        encoder=None, \n",
    "        max_len=512, \n",
    "        preprocess_pipeline=None,\n",
    "    ):\n",
    "        super().__init__(\n",
    "            TransformerName.DistilBertBaseUncased,         \n",
    "            transformer_pooling_type,\n",
    "            path_authors=None,\n",
    "            encoder=None, \n",
    "            max_len=512, \n",
    "            preprocess_pipeline=None\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ElectraSmallVectorizer(TransformerVectorizer):\n",
    "    def __init__(\n",
    "        self, \n",
    "        transformer_pooling_type,\n",
    "        path_authors=None,\n",
    "        encoder=None, \n",
    "        max_len=512, \n",
    "        preprocess_pipeline=None,\n",
    "    ):\n",
    "        super().__init__(\n",
    "            TransformerName.ElectraSmall,         \n",
    "            transformer_pooling_type,\n",
    "            path_authors=None,\n",
    "            encoder=None, \n",
    "            max_len=512, \n",
    "            preprocess_pipeline=None\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader\n",
    "from src.types.downloaded_embeddings_type import DownloadedEmbeddingType\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingVectorizer:\n",
    "    def __init__(self, embedding_type):\n",
    "        self.embedding_type = embedding_type.value\n",
    "        self.missed = 0\n",
    "        self.counter = 0\n",
    "        self.embedding_size = 0\n",
    "        self.setup()\n",
    "\n",
    "    def setup(self):\n",
    "        self.vectors = gensim.downloader.load(self.embedding_type)\n",
    "        self.embedding_size = len(self.vectors['king'])\n",
    "\n",
    "    def get_from_vectors(self, key_vectors, key):\n",
    "        self.counter += 1\n",
    "        try:\n",
    "            return key_vectors[key]\n",
    "        except:\n",
    "            self.missed += 1\n",
    "            return np.zeros(shape=(self.embedding_size, ))\n",
    "\n",
    "    def get_state(self):\n",
    "        missed, counter, accuracy = self.missed, self.counter, 100 * (self.missed / self.counter)\n",
    "        print(f\"Missed={missed}, counter={counter}, accuracy={accuracy}\")\n",
    "        return missed, counter, accuracy\n",
    "\n",
    "    def get_mean(self, corpus):\n",
    "        return [np.mean(sent, axis=0) for sent in corpus]\n",
    "\n",
    "    def fit_transform(self, X):\n",
    "        self.missed = 0\n",
    "        self.counter = 0\n",
    "\n",
    "        corpus = []\n",
    "        \n",
    "        for sentence in X:\n",
    "            tokens = sentence.split(\" \")\n",
    "            sentence_embedding = []\n",
    "            for token in tokens:\n",
    "                embedding_of_token = self.get_from_vectors(self.vectors, token)\n",
    "                sentence_embedding.append(embedding_of_token)\n",
    "            corpus.append(np.array(sentence_embedding))\n",
    "\n",
    "        return self.get_mean(corpus)\n",
    "\n",
    "    def create_embedding_matrix(self, X):\n",
    "        #TODO: add to embedding layer\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = EmbeddingVectorizer(DownloadedEmbeddingType.Glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = embedding.fit_transform(test_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 300)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(X).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 22, 18.181818181818183)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding.get_state()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings downloaded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Glove Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GloveVectorizer(EmbeddingVectorizer):\n",
    "    def __init__(self):\n",
    "        super().__init__(DownloadedEmbeddingType.Glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_vectorizer = GloveVectorizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2VecVectorizer(EmbeddingVectorizer):\n",
    "    def __init__(self):\n",
    "        super().__init__(DownloadedEmbeddingType.Word2Vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_vectorizer = Word2VecVectorizer"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "deac0217989ddcfa4345bc236e90ab22a38c5ad7d8517b867015082eaa3f672d"
  },
  "kernelspec": {
   "display_name": "Python 3.8.1 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "def adding_module_path():\n",
    "    module_path = os.path.abspath(os.path.join('..'))\n",
    "\n",
    "    if module_path not in sys.path:\n",
    "        sys.path.append(module_path)\n",
    "\n",
    "adding_module_path()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_corpus = np.array([\n",
    "    'This is the first document.',\n",
    "    'This document is the second document.',\n",
    "    'And this is the third one.',\n",
    "    'Is this the first document?',\n",
    "])\n",
    "y = np.array([0,0,0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset from=C:\\Users\\Vojta\\Desktop\\diploma\\data_test\\gutenberg\\10Authors\\Sentence3\\train.csv\n",
      "Loading dataset from=C:\\Users\\Vojta\\Desktop\\diploma\\data_test\\gutenberg\\10Authors\\Sentence3\\valid.csv\n",
      "Loading dataset from=C:\\Users\\Vojta\\Desktop\\diploma\\data_test\\gutenberg\\10Authors\\Sentence3\\test.csv\n"
     ]
    }
   ],
   "source": [
    "path_data, path_authors = get_path_to_gutenberg_sets(10, 3, PATH_TO_DATASET_FOLDER_TEST)\n",
    "train, valid, test = get_datasets(path_data, ';', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((test_corpus, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(), dtype=string, numpy=b'This is the first document.'>, <tf.Tensor: shape=(), dtype=int32, numpy=0>)\n"
     ]
    }
   ],
   "source": [
    "for x in dataset:\n",
    "    print(x)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BoW Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "BoWVectorizer = CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 1, 1, 0, 0, 1, 0, 1],\n",
       "       [0, 2, 0, 1, 0, 1, 1, 0, 1],\n",
       "       [1, 0, 0, 1, 1, 0, 1, 1, 1],\n",
       "       [0, 1, 1, 1, 0, 0, 1, 0, 1]], dtype=int64)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = BoWVectorizer()\n",
    "X = vectorizer.fit_transform(test_corpus)\n",
    "vectorizer.get_feature_names_out()\n",
    "X.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TFIDF Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "TFIDFVectorizer= TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.46979139, 0.58028582, 0.38408524, 0.        ,\n",
       "        0.        , 0.38408524, 0.        , 0.38408524],\n",
       "       [0.        , 0.6876236 , 0.        , 0.28108867, 0.        ,\n",
       "        0.53864762, 0.28108867, 0.        , 0.28108867],\n",
       "       [0.51184851, 0.        , 0.        , 0.26710379, 0.51184851,\n",
       "        0.        , 0.26710379, 0.51184851, 0.26710379],\n",
       "       [0.        , 0.46979139, 0.58028582, 0.38408524, 0.        ,\n",
       "        0.        , 0.38408524, 0.        , 0.38408524]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "vectorizer = TFIDFVectorizer()\n",
    "X = vectorizer.fit_transform(test_corpus)\n",
    "vectorizer.get_feature_names_out()\n",
    "X.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFAutoModel\n",
    "from src.types.transformer_name import TransformerName\n",
    "from src.tokenizers.transformer_tokenizer import TransformerTokenizer\n",
    "from src.encoder.create_encoder_from_path import create_encoder_from_path\n",
    "from src.tokenizers.prepare_dataset_from_tokenizer import prepare_dataset_from_tokenizer\n",
    "from src.types.transformer_pooling import TransformerPooling\n",
    "from src.models.transformer.bert_pooling_layer import BertPoolingLayer\n",
    "from transformers import AutoConfig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerVectorizer():\n",
    "    def __init__(\n",
    "        self, \n",
    "        transformer_type,\n",
    "        transformer_pooling_type,\n",
    "        path_authors=None,\n",
    "        encoder=None, \n",
    "        max_len=512, \n",
    "        preprocess_pipeline=None,\n",
    "    ):\n",
    "        self.transformer_type = transformer_type.value\n",
    "        self.transformer_pooling_type = transformer_pooling_type\n",
    "        self.encoder = encoder\n",
    "        self.max_len = max_len\n",
    "        self.preprocess_pipeline = preprocess_pipeline\n",
    "        self.path_to_authors = path_authors\n",
    "        self.setup()\n",
    "\n",
    "    def setup(self):\n",
    "        self.config = AutoConfig.from_pretrained(self.transformer_type, output_hidden_states=True)\n",
    "        self.transformer = TFAutoModel.from_config(self.config)\n",
    "        encoder = None if self.path_to_authors is None else create_encoder_from_path(self.path_to_authors)\n",
    "        self.tokenizer = TransformerTokenizer(\n",
    "            self.transformer_type, \n",
    "            encoder\n",
    "        )\n",
    "\n",
    "    def fit_transform(self, dataset):\n",
    "        #imagine X as list of sentences\n",
    "        sentence_embedding = []\n",
    "        labels = []\n",
    "\n",
    "        for x in prepare_dataset_from_tokenizer(dataset, self.tokenizer).batch(1):\n",
    "            transformer_input, label = x\n",
    "            output = self.transformer(\n",
    "                transformer_input, \n",
    "                output_hidden_states=True\n",
    "            )\n",
    "            \n",
    "            output = BertPoolingLayer()(\n",
    "                output, \n",
    "                self.transformer_pooling_type\n",
    "            )\n",
    "\n",
    "            output = output.numpy().reshape(-1)\n",
    "            label = label.numpy()[0] \n",
    "\n",
    "            labels.append(label)\n",
    "            sentence_embedding.append(output)\n",
    "            break\n",
    "        return np.array(sentence_embedding), np.array(labels)\n",
    "\n",
    "    def create_embedding_matrix(self, X):\n",
    "        #TODO: add to embedding layer\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertBaseUncasedVectorizer(TransformerVectorizer):\n",
    "    def __init__(\n",
    "        self, \n",
    "        transformer_pooling_type,\n",
    "        path_authors=None,\n",
    "        encoder=None, \n",
    "        max_len=512, \n",
    "        preprocess_pipeline=None,\n",
    "    ):\n",
    "        super().__init__(\n",
    "            TransformerName.BertBaseUncased,         \n",
    "            transformer_pooling_type,\n",
    "            path_authors=None,\n",
    "            encoder=None, \n",
    "            max_len=512, \n",
    "            preprocess_pipeline=None\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert = BertBaseUncasedVectorizer()\n",
    "X, y = bert.fit_transform(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 768)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DistilBertBaseUncasedVectorizer(TransformerVectorizer):\n",
    "    def __init__(\n",
    "        self, \n",
    "        transformer_pooling_type,\n",
    "        path_authors=None,\n",
    "        encoder=None, \n",
    "        max_len=512, \n",
    "        preprocess_pipeline=None,\n",
    "    ):\n",
    "        super().__init__(\n",
    "            TransformerName.DistilBertBaseUncased,         \n",
    "            transformer_pooling_type,\n",
    "            path_authors=None,\n",
    "            encoder=None, \n",
    "            max_len=512, \n",
    "            preprocess_pipeline=None\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ElectraSmallVectorizer(TransformerVectorizer):\n",
    "    def __init__(\n",
    "        self, \n",
    "        transformer_pooling_type,\n",
    "        path_authors=None,\n",
    "        encoder=None, \n",
    "        max_len=512, \n",
    "        preprocess_pipeline=None,\n",
    "    ):\n",
    "        super().__init__(\n",
    "            TransformerName.ElectraSmall,         \n",
    "            transformer_pooling_type,\n",
    "            path_authors=None,\n",
    "            encoder=None, \n",
    "            max_len=512, \n",
    "            preprocess_pipeline=None\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader\n",
    "from src.types.downloaded_embeddings_type import DownloadedEmbeddingType\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingVectorizer:\n",
    "    def __init__(self, embedding_type):\n",
    "        self.embedding_type = embedding_type.value\n",
    "        self.missed = 0\n",
    "        self.counter = 0\n",
    "        self.embedding_size = 0\n",
    "        self.setup()\n",
    "\n",
    "    def setup(self):\n",
    "        self.vectors = gensim.downloader.load(self.embedding_type)\n",
    "        self.embedding_size = len(self.vectors['king'])\n",
    "\n",
    "    def get_from_vectors(self, key_vectors, key):\n",
    "        self.counter += 1\n",
    "        try:\n",
    "            return key_vectors[key]\n",
    "        except:\n",
    "            self.missed += 1\n",
    "            return np.zeros(shape=(self.embedding_size, ))\n",
    "\n",
    "    def get_state(self):\n",
    "        missed, counter, accuracy = self.missed, self.counter, 100 * (self.missed / self.counter)\n",
    "        print(f\"Missed={missed}, counter={counter}, accuracy={accuracy}\")\n",
    "        return missed, counter, accuracy\n",
    "\n",
    "    def get_mean(self, corpus):\n",
    "        return [np.mean(sent, axis=0) for sent in corpus]\n",
    "\n",
    "    def fit_transform(self, X):\n",
    "        self.missed = 0\n",
    "        self.counter = 0\n",
    "\n",
    "        corpus = []\n",
    "        \n",
    "        for sentence in X:\n",
    "            tokens = sentence.split(\" \")\n",
    "            sentence_embedding = []\n",
    "            for token in tokens:\n",
    "                embedding_of_token = self.get_from_vectors(self.vectors, token)\n",
    "                sentence_embedding.append(embedding_of_token)\n",
    "            corpus.append(np.array(sentence_embedding))\n",
    "\n",
    "        return self.get_mean(corpus)\n",
    "\n",
    "    def create_embedding_matrix(self, X):\n",
    "        #TODO: add to embedding layer\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = EmbeddingVectorizer(DownloadedEmbeddingType.Glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = embedding.fit_transform(test_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 300)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(X).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 22, 18.181818181818183)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding.get_state()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings downloaded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Glove Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GloveVectorizer(EmbeddingVectorizer):\n",
    "    def __init__(self):\n",
    "        super().__init__(DownloadedEmbeddingType.Glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_vectorizer = GloveVectorizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2VecVectorizer(EmbeddingVectorizer):\n",
    "    def __init__(self):\n",
    "        super().__init__(DownloadedEmbeddingType.Word2Vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_vectorizer = Word2VecVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data_preparing.build.gutenberg_builder import GutenbergBuilder\n",
    "from src.app.project_setup import ProjectSetup\n",
    "from src.data_preparing.split.run_split_deps_on_stats import run_split_deps_on_stats_same_dir\n",
    "from src.utils.create_path_to_gutenberg import create_path_to_gutenberg_sentence_authors_sentence, create_path_to_gutenberg_authors\n",
    "from src.utils.create_test_dataset_from import create_test_dataset_from\n",
    "from src.data_preparing.split.run_split_deps_on_stats import run_split_deps_on_stats_same_dir\n",
    "from src.utils.create_path_to_gutenberg import create_path_to_gutenberg_sentence_authors_sentence\n",
    "from src.config.config import PATH_TO_DATASET_FOLDER_TEST, PATH_TO_DATASET_FOLDER_TEST, AUTHORS_FILE_NAME\n",
    "\n",
    "from src.tokenizers.prepare_dataset_from_tokenizer import prepare_dataset_from_tokenizer\n",
    "from src.tokenizers.transformer_tokenizer import TransformerTokenizer\n",
    "import tensorflow as tf\n",
    "from src.encoder.create_encoder_from_path import create_encoder_from_path\n",
    "from src.testing.get_testing_dataset import dataset as test_dataset\n",
    "from src.data_loading.get_dataset_object_from import get_dataset_object_from_path, get_datasets\n",
    "from src.utils.create_path_to_gutenberg import get_paths_to_gutenberg, get_path_to_gutenberg_sets\n",
    "\n",
    "from transformers import TFAutoModel\n",
    "from src.experiments.settings import LearningSettings\n",
    "from transformers import AutoConfig\n",
    "from src.types.transformer_name import TransformerName\n",
    "from src.types.processing_type import PreprocessingType\n",
    "from src.preprocessing.preprocessing_factory import PreprocessingFactory\n",
    "from src.types.transformer_pooling import TransformerPooling\n",
    "from src.config.run_prep import run_prep\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset from=C:\\Users\\Vojta\\Desktop\\diploma\\data_test\\gutenberg\\10Authors\\Sentence3\\train.csv\n",
      "Loading dataset from=C:\\Users\\Vojta\\Desktop\\diploma\\data_test\\gutenberg\\10Authors\\Sentence3\\valid.csv\n",
      "Loading dataset from=C:\\Users\\Vojta\\Desktop\\diploma\\data_test\\gutenberg\\10Authors\\Sentence3\\test.csv\n"
     ]
    }
   ],
   "source": [
    "model_name = TransformerName.ElectraSmall.value\n",
    "config = AutoConfig.from_pretrained(model_name, output_hidden_states=True)\n",
    "\n",
    "path_data, path_authors = get_path_to_gutenberg_sets(10, 3, PATH_TO_DATASET_FOLDER_TEST)\n",
    "train, valid, test = get_datasets(path_data, ';', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = TransformerTokenizer(\n",
    "    model_name, \n",
    "    create_encoder_from_path(\n",
    "        path_authors\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = TFAutoModel.from_config(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertPoolingLayer(tf.keras.layers.Layer):\n",
    " \n",
    "    def call(self, inputs, pooling_type):\n",
    "        if pooling_type == TransformerPooling.LastHiddenState:\n",
    "            last_hidden_state = inputs[TransformerPooling.LastHiddenState.value]\n",
    "            print(last_hidden_state)\n",
    "            return tf.reduce_mean(last_hidden_state, axis=1)\n",
    "            \n",
    "            \n",
    "        if pooling_type == TransformerPooling.Pooler:\n",
    "            pooler = inputs[TransformerPooling.Pooler.value]\n",
    "            return pooler\n",
    "        \n",
    "        \n",
    "        if pooling_type == TransformerPooling.HiddenStates:\n",
    "            #TODO: deal with hidden state\n",
    "            selector = inputs[TransformerPooling.HiddenStates.value]\n",
    "            \n",
    "            layers = tf.convert_to_tensor(selector)[-1]\n",
    "            \n",
    "            if tf.shape(tf.shape(layers)) > 3:\n",
    "                #print(layers)\n",
    "                print('A')\n",
    "                layers_together = tf.reduce_mean(layers, axis=0)\n",
    "                average_sentence_words = tf.reduce_mean(layers_together, axis=1)\n",
    "                cls = layers_together[:, 0, :]\n",
    "                return cls\n",
    "                \n",
    "                \n",
    "                #print(average_sentence_words)\n",
    "                #layers = tf.reduce_mean(layers, axis=0, keepdims=False)\n",
    "                #cls = layers[:, 0, :]\n",
    "                #average_sentence_words = tf.reduce_mean(layers[:, 1:tf.shape(layers)[1]-1, :], axis=1)\n",
    "                #return average_sentence_words\n",
    "            else:\n",
    "                print('B')\n",
    "                \n",
    "                #return tf.reduce_mean(layers, axis=1)\n",
    "                cls = layers[:, 0, :]\n",
    "                #return cls\n",
    "                average_sentence_words = tf.reduce_mean(layers[:, 1:tf.shape(layers)[1]-1, :], axis=1)\n",
    "                return average_sentence_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'pooler_output'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Vojta\\Desktop\\diploma\\jupyters\\Vectorizers.ipynb Cell 41'\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Vojta/Desktop/diploma/jupyters/Vectorizers.ipynb#ch0000039?line=0'>1</a>\u001b[0m \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m prepare_dataset_from_tokenizer(train, tokenizer)\u001b[39m.\u001b[39mbatch(\u001b[39m10\u001b[39m):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Vojta/Desktop/diploma/jupyters/Vectorizers.ipynb#ch0000039?line=1'>2</a>\u001b[0m     text, label \u001b[39m=\u001b[39m x\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Vojta/Desktop/diploma/jupyters/Vectorizers.ipynb#ch0000039?line=2'>3</a>\u001b[0m     output \u001b[39m=\u001b[39m transformer(text, output_hidden_states\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)[TransformerPooling\u001b[39m.\u001b[39;49mPooler\u001b[39m.\u001b[39;49mvalue]\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Vojta/Desktop/diploma/jupyters/Vectorizers.ipynb#ch0000039?line=3'>4</a>\u001b[0m     \u001b[39m#output = BertPoolingLayer()(output, TransformerPooling.Pooler)\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Vojta/Desktop/diploma/jupyters/Vectorizers.ipynb#ch0000039?line=4'>5</a>\u001b[0m     \u001b[39mprint\u001b[39m(output)\n",
      "File \u001b[1;32mc:\\Users\\Vojta\\Desktop\\diploma\\venv\\lib\\site-packages\\transformers\\file_utils.py:2597\u001b[0m, in \u001b[0;36mModelOutput.__getitem__\u001b[1;34m(self, k)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/Vojta/Desktop/diploma/venv/lib/site-packages/transformers/file_utils.py?line=2594'>2595</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(k, \u001b[39mstr\u001b[39m):\n\u001b[0;32m   <a href='file:///c%3A/Users/Vojta/Desktop/diploma/venv/lib/site-packages/transformers/file_utils.py?line=2595'>2596</a>\u001b[0m     inner_dict \u001b[39m=\u001b[39m {k: v \u001b[39mfor\u001b[39;00m (k, v) \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems()}\n\u001b[1;32m-> <a href='file:///c%3A/Users/Vojta/Desktop/diploma/venv/lib/site-packages/transformers/file_utils.py?line=2596'>2597</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m inner_dict[k]\n\u001b[0;32m   <a href='file:///c%3A/Users/Vojta/Desktop/diploma/venv/lib/site-packages/transformers/file_utils.py?line=2597'>2598</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   <a href='file:///c%3A/Users/Vojta/Desktop/diploma/venv/lib/site-packages/transformers/file_utils.py?line=2598'>2599</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mto_tuple()[k]\n",
      "\u001b[1;31mKeyError\u001b[0m: 'pooler_output'"
     ]
    }
   ],
   "source": [
    "for x in prepare_dataset_from_tokenizer(train, tokenizer).batch(10):\n",
    "    text, label = x\n",
    "    output = transformer(text, output_hidden_states=True)[TransformerPooling.Pooler.value]\n",
    "    #output = BertPoolingLayer()(output, TransformerPooling.Pooler)\n",
    "    print(output)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "deac0217989ddcfa4345bc236e90ab22a38c5ad7d8517b867015082eaa3f672d"
  },
  "kernelspec": {
   "display_name": "Python 3.8.1 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

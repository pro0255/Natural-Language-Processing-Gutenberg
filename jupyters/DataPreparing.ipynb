{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "661bd65d-055e-4a21-b7a3-58e2d16f1d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import os\n",
    "import json\n",
    "from enum import Enum\n",
    "from nltk import tokenize\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "2bc81eda-7bdd-4bea-85e4-2caea34e870b",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_CSV_DELIMITER = ';'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "ebba4909-ac84-4c99-b0e4-1af191e6d3a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTHOR_DIRECTORY_NAME = \"Authors\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "da136dd7-bd22-4b49-8006-07e154fbe2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GutenbergJsonAttributes(Enum):\n",
    "    Title = \"Title\"\n",
    "    Author = \"Author\"\n",
    "    Subject = \"Subject\"\n",
    "    Alias = \"Alias\"\n",
    "    Birthdate = \"Birthdate\"\n",
    "    Deathdate = \"Deathdate\"\n",
    "    Aliases = \"Aliases\"\n",
    "    Text = \"Text\"\n",
    "    Id = \"Id\"\n",
    "    AuthorId = \"AuthorId\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a1f97020-7cde-4e1b-b492-89a8421ae49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = \"*.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a5a72f17-e87d-445c-8153-9b46140a16d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_path = 'C:\\\\Users\\\\Vojta\\\\Desktop\\\\diploma\\\\gutenberg_json\\\\' + pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "55e298ef-5b48-4c6d-940e-545e7300553a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_files(path):\n",
    "    return tf.data.Dataset.list_files(path, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "33d62aec-34e4-4b20-b818-39067dd42074",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json(path):\n",
    "    with open(path) as f:\n",
    "        return json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "33fcce05-7266-4a3f-857f-96f403cb0aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate_over_gutenberg(files_path, process_func):\n",
    "    print(f\"Loading files from {files_path}\")\n",
    "    for path_to_file in tqdm(load_files(files_path)):\n",
    "        path = bytes.decode(path_to_file.numpy())\n",
    "        data = load_json(path)\n",
    "        process_func(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "74a6c03b-5af3-45f8-96e3-259665e98bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_SAVE_GENERAL_GUTENBERG = \"C:\\\\Users\\\\Vojta\\\\Desktop\\\\diploma\\\\gutenberg_from_raw\"\n",
    "PATH_TO_SAVE_DATASET = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "690e0a9e-dad2-4b6a-ab99-2bfe2ee2ec77",
   "metadata": {},
   "outputs": [],
   "source": [
    "authors = pd.read_csv(PATH_TO_ALL_AUTHORS, sep=AUTHORS_CSV_DELIMITER, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "bcf53487-d370-44bc-a53c-995a4b8eb2a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gutenberg_author_id</th>\n",
       "      <th>author.y</th>\n",
       "      <th>n</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>761</td>\n",
       "      <td>Lytton, Edward Bulwer Lytton, Baron</td>\n",
       "      <td>215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1800</td>\n",
       "      <td>Ebers, Georg</td>\n",
       "      <td>164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>53</td>\n",
       "      <td>Twain, Mark</td>\n",
       "      <td>144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8659</td>\n",
       "      <td>Kingston, William Henry Giles</td>\n",
       "      <td>132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1285</td>\n",
       "      <td>Parker, Gilbert</td>\n",
       "      <td>131</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   gutenberg_author_id                             author.y    n\n",
       "0                  761  Lytton, Edward Bulwer Lytton, Baron  215\n",
       "1                 1800                         Ebers, Georg  164\n",
       "2                   53                          Twain, Mark  144\n",
       "3                 8659        Kingston, William Henry Giles  132\n",
       "4                 1285                      Parker, Gilbert  131"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "authors.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81a34bb-cdf0-4955-b539-149fe2e446f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data_as_raw_row(data):\n",
    "    current_id = data[GutenbergJsonAttributes.Id.value]\n",
    "    copied = data.copy()\n",
    "    \n",
    "    #TODO: Maybe some kind of general processing\n",
    "    processed_text = copied[GutenbergJsonAttributes.Text.value][0]\n",
    "\n",
    "    copied[GutenbergJsonAttributes.Text.value] = [processed_text]\n",
    "    \n",
    "    path = PATH_TO_SAVE_GENERAL_GUTENBERG + \"\\\\\" + str(current_id[0]) + \".json\"\n",
    "    \n",
    "    with open(path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(copied, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0e41b16e-517f-4b37-91da-384335749350",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'process_data_as_raw_row' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [22]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[0m iterate_over_gutenberg(raw_path, \u001b[43mprocess_data_as_raw_row\u001b[49m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'process_data_as_raw_row' is not defined"
     ]
    }
   ],
   "source": [
    "iterate_over_gutenberg(raw_path, process_data_as_raw_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e22a43a7-2078-4848-8ca4-e57118280d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_newlines(document):\n",
    "    return document.replace('\\n', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "81d93158-aa50-4610-909e-ac0eb4c4ae78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_project_delimiter(document):\n",
    "    return document.replace(PROJECT_CSV_DELIMITER, '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1f505c60-a0e6-461d-8947-dda814c25c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_document_by_sentence(document, k):\n",
    "    preprocessed_document_for_sentences = preprocess_newlines(document)\n",
    "    preprocessed_document_for_sentences = preprocess_project_delimiter(preprocessed_document_for_sentences)\n",
    "    sentences = tokenize.sent_tokenize(preprocessed_document_for_sentences)\n",
    "    chunked_sentences = [' '.join(sentences[sent_index:sent_index+k]) for sent_index in range(0, len(sentences), k)]\n",
    "    return chunked_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1d55af2e-4b32-4eb9-8153-245dad5fb75d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_process_func(k, name, path, authors_tuple):\n",
    "    number_of_authors = len(authors_tuple)\n",
    "    authors_ids, authors_names = zip(*authors_tuple)\n",
    "    \n",
    "    directory_for_file = os.sep.join([path, f'{number_of_authors}Authors', name])\n",
    "    \n",
    "    if not os.path.exists(directory_for_file):\n",
    "        os.makedirs(directory_for_file)\n",
    "    \n",
    "    authors_dataframe = pd.DataFrame.from_dict({\n",
    "        GutenbergJsonAttributes.AuthorId.value: authors_ids, \n",
    "        GutenbergJsonAttributes.Author.value: authors_names\n",
    "    },)\n",
    "    \n",
    "    \n",
    "    name_of_authors_file = \"authors.csv\"\n",
    "    authors_save = os.sep.join([directory_for_file, name_of_authors_file])\n",
    "                               \n",
    "    print(f'Saving authors csv to {authors_save}')\n",
    "                               \n",
    "    authors_dataframe.to_csv(authors_save, index=False, sep=';')\n",
    "    \n",
    "    def process_to_create_dataset(data):\n",
    "        current_author = data[GutenbergJsonAttributes.Author.value][0] \n",
    "        current_text = data[GutenbergJsonAttributes.Text.value][0]\n",
    "        current_author_id = data[GutenbergJsonAttributes.AuthorId.value][0] \n",
    "        \n",
    "        is_required_author = current_author_id in authors_ids\n",
    "        \n",
    "        if is_required_author:\n",
    "            \n",
    "            chunked_sentences = chunk_document_by_sentence(current_text, k)\n",
    "\n",
    "            name_of_file = \"data.csv\"\n",
    "            with open(os.sep.join([directory_for_file, name_of_file]), 'a', newline='') as f:\n",
    "                writer = csv.writer(f, delimiter=';')\n",
    "\n",
    "                for chunk in chunked_sentences:\n",
    "                    value = [chunk, current_author_id]\n",
    "                    writer.writerow(value)\n",
    "\n",
    "            return chunked_sentences\n",
    "        \n",
    "        return []\n",
    "\n",
    "    return process_to_create_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1c0e7ef7-9252-407d-8aec-d02175285f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "DIRECTORY_TO_SAVE = \"C:\\\\Users\\\\Vojta\\\\Desktop\\\\diploma\\\\data\\\\gutenberg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5fcd56d5-bdb5-4b9c-8258-7a513ba057bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_ALL_AUTHORS = \"C:\\\\Users\\\\Vojta\\\\Desktop\\\\diploma\\\\gutenberg_downloaded\\\\authors\\\\authors.csv\"\n",
    "AUTHORS_CSV_DELIMITER = \",\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fa3c541b-c96b-4aea-be0b-ee1d3e5a925b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AuthorsGenerator:\n",
    "    def __init__(self, path: str, sep: str):\n",
    "        self.path = path\n",
    "        self.sep = sep\n",
    "    \n",
    "    def generate_top_k(self, k: int):\n",
    "        data = pd.read_csv(self.path, sep=self.sep)\n",
    "        rows = data.shape[0]\n",
    "        authors_ids = data.iloc[0:rows-1, 0].astype(int).values[0:k]\n",
    "        authors_names = data.iloc[0:rows-1, 1].astype(str).values[0:k]\n",
    "        authors_tuple = list(zip(authors_ids, authors_names))\n",
    "        del data\n",
    "        return authors_tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4acd89ec-2555-4c38-8fdd-2e16a29e6c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "authors_generator = AuthorsGenerator(PATH_TO_ALL_AUTHORS, AUTHORS_CSV_DELIMITER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "940702b5-c12c-436d-9350-f2dd00428558",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_SAVE_TEST = \"C:\\\\Users\\\\Vojta\\\\Desktop\\\\diploma\\\\gutenberg_test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d4aa1d32-87e5-4b55-920f-a329d216a4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_path = os.sep.join([PATH_TO_SAVE_TEST, pattern])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b78e368d-26c7-492c-b3fa-458b68f1ad64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving authors csv to C:\\Users\\Vojta\\Desktop\\diploma\\data\\gutenberg\\10Authors\\Sentence3\\authors.csv\n",
      "Loading files from C:\\Users\\Vojta\\Desktop\\diploma\\gutenberg_test\\*.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 111.13it/s]\n"
     ]
    }
   ],
   "source": [
    "iterate_over_gutenberg(test_path, build_process_func(3, \"Sentence\" + str(3), DIRECTORY_TO_SAVE, authors_generator.generate_top_k(10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b28e066-cd51-43b6-b5b5-34e3dfa03f0a",
   "metadata": {},
   "source": [
    "# Have big csv ... iterate over csv and create test train valid sets deps on statistcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "bc8efd48-9ece-438e-9f38-4efe1e00ba94",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_DATASET_FOLDER = \"C:\\\\Users\\\\Vojta\\\\Desktop\\\\diploma\\\\data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8c155d97-e9e9-44ee-a147-30553fc51417",
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "import os.path\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "63aa7d8b-eace-4f36-bbdc-d6c5afc0f663",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataSet(Enum):\n",
    "    Gutenberg = \"gutenberg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a12b40d0-2526-44d1-b5ef-7025818d77ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataSetType(Enum):\n",
    "    Sentence = \"Sentence\"\n",
    "    Article = \"Article\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "332d4019-4ef6-45c3-8bc6-5d69bb474d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_NAME = 'data.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "e18eb81b-bad1-4da0-8669-7475a3340a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_path(directory, dataset, authors_directory, dataset_type, k=None):\n",
    "    is_sentence_type = dataset_type == DataSetType.Sentence \n",
    "    if is_sentence_type and k is None:\n",
    "        raise Exception(f\"Sentence should be specified with k argument!\")\n",
    "    \n",
    "    return os.path.join(directory, dataset.value, authors_directory, dataset_type.value + str(k), DATA_NAME) if is_sentence_type else os.path.join(directory, dataset.value, authors_directory, dataset_type.value, DATA_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "d12a3538-171e-4b22-be4b-3530922e9efb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Vojta\\\\Desktop\\\\diploma\\\\data\\\\gutenberg\\\\10Authors\\\\Sentence10\\\\data.csv'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_path(PATH_TO_DATASET_FOLDER, DataSet.Gutenberg, create_author_directory(10), DataSetType.Sentence, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "3bbbe1b5-ca89-48e8-a5d1-36af40d446fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(text):\n",
    "    return text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f59b822f-2853-4dc4-bc9e-cba4c80b2d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset_object_from_path(csv_filename, delim, text_pipeline_func=None):\n",
    "    dataset = tf.data.TextLineDataset(filenames=csv_filename)\n",
    "    \n",
    "    def parse_csv(line):\n",
    "        csv_line = bytes.decode(line.numpy())\n",
    "        text, author = csv_line.split(delim)\n",
    "        if text_pipeline_func is not None:\n",
    "            text = text_pipeline_func(text)\n",
    "        return text, author \n",
    "\n",
    "    dataset = dataset.map(lambda tpl: tf.py_function(parse_csv, [tpl], [tf.string, tf.string]))\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c7ba844a-6435-4d4a-bb84-add814c3b496",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = get_dataset_object_from_path(create_path(PATH_TO_DATASET_FOLDER, DataSet.Gutenberg, DataSetType.Sentence, 3), ';', process_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a97fec-4844-4828-9dc0-c7033899658d",
   "metadata": {},
   "source": [
    "# Create statistics for specified dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "1f963711-2e66-42c2-9d7d-53f17b7ccc74",
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME_OF_STATISTICS_FILE = 'stats_'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "b6ec9fc7-f23c-4f92-8889-9ccd81188013",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_stats_file_name(name_of_file):\n",
    "    return f'{NAME_OF_STATISTICS_FILE}{name_of_file}.xlsx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "74356449-5821-473c-9362-6468df0de41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_input_for_statistics(path, sep, metric_instances = [] ,text_pipeline_func = None, save = True):\n",
    "    path_parts = path.split(os.path.sep)\n",
    "    name_of_file = path_parts[-1]\n",
    "    del path_parts[-1]\n",
    "    path_parts.append(create_stats_file_name(name_of_file))\n",
    "    path_to_save = os.path.sep.join(path_parts)\n",
    "    return get_dataset_object_from_path(path, sep, text_pipeline_func), MetricWrapper(metric_instances, path_to_save if save else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "14c5b32a-b209-4f25-b833-22ad48ce762e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#build_input_for_statistics(create_path(PATH_TO_DATASET_FOLDER, DataSet.Gutenberg, DataSetType.Sentence, 3), ';', process_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba1b75c9-4314-4b3a-bec6-4fcda930de39",
   "metadata": {},
   "source": [
    "### TODO: create more metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "d94223f3-fb5c-4114-978d-d8000b75faf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_author_directory(k):\n",
    "    return f'{k}{AUTHOR_DIRECTORY_NAME}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "5e9e6d6c-3828-4c4f-9da6-12425ebd96c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelMetric:\n",
    "    def __init__(self):\n",
    "        self.state = {}\n",
    "    \n",
    "    def update_state(self, text, label):\n",
    "        self.state[label] = self.state.get(label, 0) + 1\n",
    "        \n",
    "    def get_dataframe(self):\n",
    "        return pd.DataFrame.from_dict(self.state, orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "67d4a2a0-75f5-4764-b9b5-ad1541fa27cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetricWrapper:\n",
    "    def __init__(self, metric_instances = [], path_to_save = None):\n",
    "        self.path_to_save = path_to_save\n",
    "        self.metric_instances = metric_instances\n",
    "    \n",
    "    def update_state(self, text, label):\n",
    "        #TODO: Can derive from this class and update __init__ and update_states\n",
    "        for instance in self.metric_instances:\n",
    "            instance.update_state(text, label)\n",
    "    \n",
    "    def process_row(self, record):\n",
    "        text, label = record\n",
    "        text = bytes.decode(text.numpy())\n",
    "        label = bytes.decode(label.numpy())\n",
    "        self.update_state(text, label)\n",
    "    \n",
    "    def save(self):\n",
    "        if self.path_to_save is not None:\n",
    "            print(f'Saving to {self.path_to_save}')\n",
    "            with pd.ExcelWriter(self.path_to_save, engine='xlsxwriter') as writer:\n",
    "                for metric_instance in self.metric_instances:\n",
    "                    metric_instance.get_dataframe().to_excel(writer, sheet_name=type(metric_instance).__name__)            \n",
    "        else:\n",
    "            print('Saving path is not specified!')\n",
    "            \n",
    "    def __str__(self):\n",
    "        string = ''\n",
    "        for instance in self.metric_instances:\n",
    "            string += type(instance).__name__\n",
    "            string += instance.get_dataframe().to_string()\n",
    "            string += '\\n'\n",
    "        return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "3e8f71e2-76dc-4c44-9e7d-ed73e33e4f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_statistics_from(dataset, metric_instance):\n",
    "    for i, record in enumerate(dataset):\n",
    "        metric_instance.process_row(record)\n",
    "    metric_instance.save()\n",
    "    return metric_instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "1dcfdac3-e6ec-4797-b444-feb1c8f1b6fb",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'value'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [123]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m metric_instance \u001b[38;5;241m=\u001b[39m create_statistics_from(\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;241m*\u001b[39mbuild_input_for_statistics(\n\u001b[1;32m----> 3\u001b[0m         \u001b[43mcreate_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43mPATH_TO_DATASET_FOLDER\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDataSet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mGutenberg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDataSetType\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSentence\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m, \n\u001b[0;32m      4\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m;\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[0;32m      5\u001b[0m         [LabelMetric()], \n\u001b[0;32m      6\u001b[0m         process_text,\n\u001b[0;32m      7\u001b[0m         \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m      8\u001b[0m     )\n\u001b[0;32m      9\u001b[0m )\n",
      "Input \u001b[1;32mIn [70]\u001b[0m, in \u001b[0;36mcreate_path\u001b[1;34m(directory, dataset, authors_directory, dataset_type, k)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_sentence_type \u001b[38;5;129;01mand\u001b[39;00m k \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSentence should be specified with k argument!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(directory, dataset\u001b[38;5;241m.\u001b[39mvalue, authors_directory, dataset_type\u001b[38;5;241m.\u001b[39mvalue \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(k), DATA_NAME) \u001b[38;5;28;01mif\u001b[39;00m is_sentence_type \u001b[38;5;28;01melse\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(directory, dataset\u001b[38;5;241m.\u001b[39mvalue, authors_directory, \u001b[43mdataset_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m, DATA_NAME)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'int' object has no attribute 'value'"
     ]
    }
   ],
   "source": [
    "metric_instance = create_statistics_from(\n",
    "    *build_input_for_statistics(\n",
    "        create_path(PATH_TO_DATASET_FOLDER, DataSet.Gutenberg, DataSetType.Sentence, 3), \n",
    "        ';', \n",
    "        [LabelMetric()], \n",
    "        process_text,\n",
    "        True\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "df62230d-6575-4024-b861-161f147e893c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'metric_instance' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [124]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mmetric_instance\u001b[49m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'metric_instance' is not defined"
     ]
    }
   ],
   "source": [
    "print(metric_instance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "80d91413-f885-479c-b117-3549a490570e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_file_to_train_test_valid(path_to_file, train_size=1, test_size=0, valid_size=0, min_label_size=None):\n",
    "    print(train_size, test_size, valid_size, path_to_file, min_label_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "26a968cd-3024-4ab5-a1ab-16d7646b98e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0 0 C:\\Users\\Vojta\\Desktop\\diploma\\data\\gutenberg\\10Authors\\Sentence3\\data.csv None\n"
     ]
    }
   ],
   "source": [
    "split_file_to_train_test_valid(\n",
    "    create_path(PATH_TO_DATASET_FOLDER, DataSet.Gutenberg, create_author_directory(10), DataSetType.Sentence, 3),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b53f120f-59ea-46e3-8660-ec51e5b3efd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_DATASET_FOLDER = \"C:\\\\Users\\\\Vojta\\\\Desktop\\\\diploma\\\\data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89c33387-3c8b-4807-8f5c-2c1acf9caaba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "import os.path\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "470dc4dd-8d86-43ed-938a-685ceeb289b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataSet(Enum):\n",
    "    Gutenberg = \"gutenberg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe6b9729-e452-4453-b67f-699986d4062d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataSetType(Enum):\n",
    "    Sentence = \"Sentence\"\n",
    "    Article = \"Article\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "157e3c15-0b36-4db3-bfd3-91dff0bf872b",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_NAME = 'data.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b304c3e4-4264-4f69-a328-b025d6d87463",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_path(directory, dataset, dataset_type, k=None):\n",
    "    is_sentence_type = dataset_type == DataSetType.Sentence \n",
    "    if is_sentence_type and k is None:\n",
    "        raise Exception(f\"Sentence should be specified with k argument!\")\n",
    "    \n",
    "    return os.path.join(directory, dataset.value, dataset_type.value + str(k), DATA_NAME) if is_sentence_type else os.path.join(directory, dataset.value, dataset_type.value, DATA_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "41ca9401-cbc7-4db5-96af-6dd993473afa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Vojta\\\\Desktop\\\\diploma\\\\data\\\\gutenberg\\\\Sentence10\\\\data.csv'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_path(PATH_TO_DATASET_FOLDER, DataSet.Gutenberg, DataSetType.Sentence, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "08715b6b-524a-4f3e-afd6-9a6ec2a056c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(text):\n",
    "    return text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3ccc15e1-f038-4f4a-b2d0-9a9542e964b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset_object_from_path(csv_filename, delim, text_pipeline_func=None):\n",
    "    dataset = tf.data.TextLineDataset(filenames=csv_filename)\n",
    "    \n",
    "    def parse_csv(line):\n",
    "        csv_line = bytes.decode(line.numpy())\n",
    "        text, author = csv_line.split(delim)\n",
    "        if text_pipeline_func is not None:\n",
    "            text = text_pipeline_func(text)\n",
    "        return text, author \n",
    "\n",
    "    dataset = dataset.map(lambda tpl: tf.py_function(parse_csv, [tpl], [tf.string, tf.string]))\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dcb2804a-9b12-419c-8370-c418724d1a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = get_dataset_object_from_path(create_path(PATH_TO_DATASET_FOLDER, DataSet.Gutenberg, DataSetType.Sentence, 3), ';', process_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9bf8f2-f8ef-4ce7-8586-719c87304643",
   "metadata": {},
   "source": [
    "# Create statistics for specified dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "da798cdf-faf6-491e-a74d-cb1d75a3c52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME_OF_STATISTICS_FILE = 'stats_'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b65a0db4-3f24-459c-a46c-20f05c67127a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_stats_file_name(name_of_file):\n",
    "    return f'{NAME_OF_STATISTICS_FILE}{name_of_file}.xlsx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "777f0997-8917-4aea-9b97-35f16470e0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_input_for_statistics(path, sep, metric_instances = [] ,text_pipeline_func = None, save = True):\n",
    "    path_parts = path.split(os.path.sep)\n",
    "    name_of_file = path_parts[-1]\n",
    "    del path_parts[-1]\n",
    "    path_parts.append(create_stats_file_name(name_of_file))\n",
    "    path_to_save = os.path.sep.join(path_parts)\n",
    "    return get_dataset_object_from_path(path, sep, text_pipeline_func), MetricWrapper(metric_instances, path_to_save if save else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a7de2653-63de-4813-a225-e419e8d4f37c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<MapDataset shapes: (<unknown>, <unknown>), types: (tf.string, tf.string)>,\n",
       " <__main__.MetricWrapper at 0x22005ca2430>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "build_input_for_statistics(create_path(PATH_TO_DATASET_FOLDER, DataSet.Gutenberg, DataSetType.Sentence, 3), ';', process_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776e1c0c-17cd-48fd-8aaa-759d6f541673",
   "metadata": {},
   "source": [
    "### TODO: create more metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "933ed869-a3f8-4ece-9c8d-fecb6bfbf95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelMetric:\n",
    "    def __init__(self):\n",
    "        self.state = {}\n",
    "    \n",
    "    def update_state(self, text, label):\n",
    "        self.state[label] = self.state.get(label, 0) + 1\n",
    "        \n",
    "    def get_dataframe(self):\n",
    "        return pd.DataFrame.from_dict(self.state, orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "863821cf-6c2f-42c3-ba32-b90bf540ba19",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetricWrapper:\n",
    "    def __init__(self, metric_instances = [], path_to_save = None):\n",
    "        self.path_to_save = path_to_save\n",
    "        self.metric_instances = metric_instances\n",
    "    \n",
    "    def update_state(self, text, label):\n",
    "        #TODO: Can derive from this class and update __init__ and update_states\n",
    "        for instance in self.metric_instances:\n",
    "            instance.update_state(text, label)\n",
    "    \n",
    "    def process_row(self, record):\n",
    "        text, label = record\n",
    "        text = bytes.decode(text.numpy())\n",
    "        label = bytes.decode(label.numpy())\n",
    "        self.update_state(text, label)\n",
    "    \n",
    "    def save(self):\n",
    "        if self.path_to_save is not None:\n",
    "            print(f'Saving to {self.path_to_save}')\n",
    "            with pd.ExcelWriter(self.path_to_save, engine='xlsxwriter') as writer:\n",
    "                for metric_instance in self.metric_instances:\n",
    "                    metric_instance.get_dataframe().to_excel(writer, sheet_name=type(metric_instance).__name__)            \n",
    "        else:\n",
    "            print('Saving path is not specified!')\n",
    "            \n",
    "    def __str__(self):\n",
    "        string = ''\n",
    "        for instance in self.metric_instances:\n",
    "            string += type(instance).__name__\n",
    "            string += instance.get_dataframe().to_string()\n",
    "            string += '\\n'\n",
    "        return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fca2d5da-e075-4937-a03f-7be5b9850597",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_statistics_from(dataset, metric_instance):\n",
    "    for i, record in enumerate(dataset):\n",
    "        metric_instance.process_row(record)\n",
    "    metric_instance.save()\n",
    "    return metric_instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8d7d0de9-90bf-48e7-a3f4-f50a47047915",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving to C:\\Users\\Vojta\\Desktop\\diploma\\data\\gutenberg\\Sentence3\\stats_data.csv.xlsx\n"
     ]
    }
   ],
   "source": [
    "metric_instance = create_statistics_from(\n",
    "    *build_input_for_statistics(\n",
    "        create_path(PATH_TO_DATASET_FOLDER, DataSet.Gutenberg, DataSetType.Sentence, 3), \n",
    "        ';', \n",
    "        [LabelMetric()], \n",
    "        process_text,\n",
    "        True\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1045bd37-3ec4-4aa2-99c4-52a84d6a9858",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LabelMetric                                    0\n",
      "Twain, Mark                      3933\n",
      "Meredith, George                  374\n",
      "Jacobs, W. W. (William Wymark)  18356\n",
      "Fenn, George Manville             302\n",
      "Balzac, Honoré de                 495\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metric_instance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "7cedd908-f5b6-4583-8b70-43f941264923",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_SIZE = 0.7\n",
    "VALIDATION_SIZE = 0.15\n",
    "TEST_SIZE = 0.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "ebdaacbe-23e5-48f6-b5dd-645197feef17",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_NAME = 'train.csv'\n",
    "TEST_NAME = 'test.csv'\n",
    "VALIDATION_NAME = 'valid.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "33247a7e-c2a9-4131-8ff9-1ab141ebc7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_size(train_size=1, test_size=0, valid_size=0):\n",
    "    if np.sum([train_size, test_size, valid_size]) != 1:\n",
    "        assert Exception(\"Is not valid split!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "id": "337c232c-8635-4354-a1fc-3762723d9f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_from(path):\n",
    "    if os.path.exists(path):\n",
    "        os.remove(path)\n",
    "        print(f'Deleting file {path}')\n",
    "    else:\n",
    "        print(f'File {path} do not exists')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "id": "ca6cd831-464c-4a0e-9a74-f40bcf920791",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataSetSplitter:\n",
    "    def __init__(self, path_to_save, train_size, test_size, valid_size, label_counter, normalization_size):\n",
    "        self.directory_path = path_to_save\n",
    "        self.size_file_name = 'subset_sizes.csv'\n",
    "        self.create_state(train_size, test_size, valid_size)\n",
    "        self.delete_files_in_directory()\n",
    "        self.counter_dataframe = label_counter.copy()\n",
    "        self.normalize(normalization_size)  \n",
    "        self.prepare_counters()\n",
    "        self.save_counters()\n",
    "\n",
    "        \n",
    "    def create_state(self, train_size, test_size, valid_size):\n",
    "        self.state = {\n",
    "            'train': {\n",
    "                'path': os.path.sep.join([self.directory_path, TRAIN_NAME]),\n",
    "                'size': train_size\n",
    "            \n",
    "            },\n",
    "            'test': {\n",
    "                'path': os.path.sep.join([self.directory_path, TEST_NAME]),\n",
    "                'size': test_size\n",
    "            \n",
    "            },\n",
    "            'valid': {\n",
    "                'path': os.path.sep.join([self.directory_path, VALIDATION_NAME]),\n",
    "                'size': valid_size\n",
    "            \n",
    "            },\n",
    "        }\n",
    "        \n",
    "    def normalize(self, normalization_size):\n",
    "        if normalization_size is not None:\n",
    "            self.counter_dataframe.iloc[:, 0] =  normalization_size\n",
    "               \n",
    "    def prepare_counters(self):\n",
    "        dataset_counters = {name:{} for name in self.state.keys()}\n",
    "    \n",
    "        for key in dataset_counters.keys():\n",
    "            set_key_counter = {}\n",
    "            for author_id, row in self.counter_dataframe.iterrows():\n",
    "                count = row[0]\n",
    "                set_key_counter[author_id] = math.floor(count * self.state[key]['size'])\n",
    "            dataset_counters[key] = set_key_counter\n",
    "    \n",
    "        \n",
    "        self.dataset_counters = dataset_counters\n",
    "        \n",
    "    def get_path(self, label):\n",
    "        picks = []\n",
    "    \n",
    "        for key in self.dataset_counters.keys():\n",
    "            if self.dataset_counters[key][label] > 0:\n",
    "                picks.append(key)\n",
    "        \n",
    "        if len(picks) == 0:\n",
    "            return None\n",
    "        #choice one\n",
    "        pick = random.choice(picks)\n",
    "        #subtract\n",
    "        self.dataset_counters[pick][label] -= 1\n",
    "        #return path according to pick\n",
    "        return self.state[pick]['path']\n",
    "    \n",
    "    def save_counters(self):\n",
    "        counters = pd.DataFrame.from_dict(self.dataset_counters, orient='index')\n",
    "        counters.to_csv(os.path.sep.join([self.directory_path, self.size_file_name]), sep=';')\n",
    "        \n",
    "        \n",
    "    def delete_files_in_directory(self):\n",
    "        for v in self.state.values():\n",
    "            current_subset_path = v['path']\n",
    "            delete_from(current_subset_path)\n",
    "        delete_from(os.path.sep.join([self.directory_path, self.size_file_name]))\n",
    "                \n",
    "                \n",
    "    def build_subsets(self, dataset):\n",
    "        for line in tqdm(dataset.shuffle(10000).as_numpy_iterator()):\n",
    "            #label = author Id\n",
    "            text, label = line\n",
    "            text = bytes.decode(text)\n",
    "            \n",
    "            #TODO: DELETE!\n",
    "            label = bytes.decode(label)\n",
    "            \n",
    "            path = self.get_path(label)\n",
    "            \n",
    "            if path is None:\n",
    "                continue\n",
    "                \n",
    "            with open(path, 'a', newline='') as f:\n",
    "                writer = csv.writer(f, delimiter=';')\n",
    "                value = [text, label]\n",
    "                writer.writerow(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "id": "aa49b828-b6d0-4b7b-85e2-90cc4a8523bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_file_to_train_test_valid(\n",
    "    path_to_load, \n",
    "    path_to_save, \n",
    "    label_metric=None,\n",
    "    normalization_size=None,\n",
    "    train_size=TRAIN_SIZE, \n",
    "    test_size=TEST_SIZE, \n",
    "    valid_size=VALIDATION_SIZE,\n",
    "):\n",
    "    check_size(train_size, test_size, valid_size)\n",
    "    splitter = DataSetSplitter(path_to_save, train_size, test_size, valid_size, label_metric, normalization_size)\n",
    "    \n",
    "    dataset = get_dataset_object_from_path(path_to_load, ';', None)\n",
    "    splitter.build_subsets(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "id": "99e2110d-148b-4b23-8353-5c415330cff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_split_deps_on_stats(path_to_load, path_to_save, normalization = True, train_size=TRAIN_SIZE, test_size=TEST_SIZE, valid_size=VALIDATION_SIZE):\n",
    "    check_size(train_size, test_size, valid_size)\n",
    "    \n",
    "    metric_instance = create_statistics_from(\n",
    "        *build_input_for_statistics(\n",
    "            path_to_load,\n",
    "            ';', \n",
    "            [LabelMetric()], \n",
    "            process_text,\n",
    "            False\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    number_of_min_label = None\n",
    "    label_metric = metric_instance.metric_instances[0].get_dataframe()\n",
    "    \n",
    "    if normalization:\n",
    "        sorted_label_metric_frame = label_metric.sort_values(by=0)\n",
    "        id_of_min_label = sorted_label_metric_frame.iloc[0].name\n",
    "        number_of_min_label = sorted_label_metric_frame.iloc[0][0]\n",
    "    \n",
    "    split_file_to_train_test_valid(path_to_load, path_to_save, label_metric, number_of_min_label)\n",
    "    return label_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "id": "7349006b-abdd-4d16-a953-03af2679c1af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Vojta\\\\Desktop\\\\diploma\\\\data\\\\gutenberg\\\\10Authors\\\\Sentence3\\\\data.csv'"
      ]
     },
     "execution_count": 429,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = create_path(PATH_TO_DATASET_FOLDER, DataSet.Gutenberg, create_author_directory(10), DataSetType.Sentence, 3)\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "id": "83f36a40-449f-4866-804c-d5871746a9b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving path is not specified!\n",
      "Deleting file C:\\Users\\Vojta\\Desktop\\diploma\\data\\gutenberg\\10Authors\\Sentence3\\train.csv\n",
      "Deleting file C:\\Users\\Vojta\\Desktop\\diploma\\data\\gutenberg\\10Authors\\Sentence3\\test.csv\n",
      "Deleting file C:\\Users\\Vojta\\Desktop\\diploma\\data\\gutenberg\\10Authors\\Sentence3\\valid.csv\n",
      "Deleting file C:\\Users\\Vojta\\Desktop\\diploma\\data\\gutenberg\\10Authors\\Sentence3\\subset_sizes.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23460it [00:23, 999.02it/s] \n"
     ]
    }
   ],
   "source": [
    "res = run_split_deps_on_stats(\n",
    "    p,\n",
    "    os.path.sep.join(p.split(os.path.sep)[0:-1]),\n",
    "    False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c42cd17-fef1-44b7-a502-47fa1674c1d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f75eed4-3d87-4f28-af4f-72517308df5e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

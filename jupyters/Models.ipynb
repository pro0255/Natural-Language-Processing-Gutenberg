{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "def adding_module_path():\n",
    "    module_path = os.path.abspath(os.path.join('..'))\n",
    "\n",
    "    if module_path not in sys.path:\n",
    "        sys.path.append(module_path)\n",
    "\n",
    "adding_module_path()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data_preparing.build.gutenberg_builder import GutenbergBuilder\n",
    "from src.app.project_setup import ProjectSetup\n",
    "from src.data_preparing.split.run_split_deps_on_stats import run_split_deps_on_stats_same_dir\n",
    "from src.utils.create_path_to_gutenberg import create_path_to_gutenberg_sentence_authors_sentence, create_path_to_gutenberg_authors\n",
    "from src.utils.create_test_dataset_from import create_test_dataset_from\n",
    "from src.data_preparing.split.run_split_deps_on_stats import run_split_deps_on_stats_same_dir\n",
    "from src.utils.create_path_to_gutenberg import create_path_to_gutenberg_sentence_authors_sentence\n",
    "from src.config.config import PATH_TO_DATASET_FOLDER_TEST, PATH_TO_DATASET_FOLDER_TEST, AUTHORS_FILE_NAME\n",
    "\n",
    "from src.tokenizers.prepare_dataset_from_tokenizer import prepare_dataset_from_tokenizer\n",
    "from src.tokenizers.transformer_tokenizer import TransformerTokenizer\n",
    "import tensorflow as tf\n",
    "from src.encoder.create_encoder_from_path import create_encoder_from_path\n",
    "from src.testing.get_testing_dataset import dataset as test_dataset\n",
    "from src.data_loading.get_dataset_object_from import get_dataset_object_from_path\n",
    "from src.utils.create_path_to_gutenberg import get_paths_to_gutenberg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFAutoModel\n",
    "from src.experiments.settings import LearningSettings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "settings = LearningSettings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_name = \"bert-base-uncased\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "path_data, path_authors = get_paths_to_gutenberg(10, 3, PATH_TO_DATASET_FOLDER_TEST)\n",
    "dataset = get_dataset_object_from_path(path_data, ';', None)\n",
    "\n",
    "\n",
    "# tokenizer = TransformerTokenizer(\n",
    "#     model_name, \n",
    "#     create_encoder_from_path(\n",
    "#         path_authors\n",
    "#     )\n",
    "# )\n",
    "\n",
    "tokenizer = TransformerTokenizer(\n",
    "    model_name, \n",
    "    None\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "transformer = TFAutoModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tf.keras.layers.Input(shape=(512, ), name='input_ids', dtype='int32')\n",
    "mask = tf.keras.layers.Input(shape=(512, ), name='attention_mask', dtype='int32')\n",
    "\n",
    "embeddings = transformer(input_ids, attention_mask=mask)[0]  # we only keep tensor 0 (last_hidden_state)\n",
    "\n",
    "X = tf.keras.layers.GlobalMaxPool1D()(embeddings)  # reduce tensor dimensionality\n",
    "X = tf.keras.layers.BatchNormalization()(X)\n",
    "X = tf.keras.layers.Dense(128, activation='relu')(X)\n",
    "X = tf.keras.layers.Dropout(0.1)(X)\n",
    "y = tf.keras.layers.Dense(10, activation='softmax', name='outputs')(X)  # adjust based on number of sentiment classes\n",
    "\n",
    "model = tf.keras.Model(inputs=[input_ids, mask], outputs=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers[2].trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_ids (InputLayer)         [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " attention_mask (InputLayer)    [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " tf_bert_model (TFBertModel)    TFBaseModelOutputWi  109482240   ['input_ids[0][0]',              \n",
      "                                thPoolingAndCrossAt               'attention_mask[0][0]']         \n",
      "                                tentions(last_hidde                                               \n",
      "                                n_state=(None, 512,                                               \n",
      "                                 768),                                                            \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 past_key_values=No                                               \n",
      "                                ne, hidden_states=N                                               \n",
      "                                one, attentions=Non                                               \n",
      "                                e, cross_attentions                                               \n",
      "                                =None)                                                            \n",
      "                                                                                                  \n",
      " global_max_pooling1d (GlobalMa  (None, 768)         0           ['tf_bert_model[0][0]']          \n",
      " xPooling1D)                                                                                      \n",
      "                                                                                                  \n",
      " batch_normalization (BatchNorm  (None, 768)         3072        ['global_max_pooling1d[0][0]']   \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 128)          98432       ['batch_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " dropout_37 (Dropout)           (None, 128)          0           ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " outputs (Dense)                (None, 10)           1290        ['dropout_37[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 109,585,034\n",
      "Trainable params: 101,258\n",
      "Non-trainable params: 109,483,776\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=settings.loss, optimizer=settings.optimizer, metrics=settings.metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "settings.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "settings.epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(prepare_dataset_from_tokenizer(test_dataset, tokenizer), epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = model.predict(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'input_ids': <tf.Tensor: shape=(3, 512), dtype=int32, numpy=\n",
      "array([[ 101, 7592, 1045, ...,    0,    0,    0],\n",
      "       [ 101, 1045, 2066, ...,    0,    0,    0],\n",
      "       [ 101, 3231, 2182, ...,    0,    0,    0]])>, 'attention_mask': <tf.Tensor: shape=(3, 512), dtype=int32, numpy=\n",
      "array([[1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0]])>}, <tf.Tensor: shape=(3,), dtype=int32, numpy=array([12, 15, 20])>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for x in prepare_dataset_from_tokenizer(test_dataset, tokenizer).batch(32):\n",
    "    print(x)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run model experiment with to lower\n",
    "\n",
    "... find best preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test ideas\n",
    "\n",
    "- one dataset - combination of preprocessing\n",
    "- size of input (k) - deps on bert - make statistics\n",
    "- type of transformer - distilbert, larbe bert etc\n",
    "- head of transformer - \n",
    "- encoding of transformer\n",
    "- classic model of tranformer\n",
    "- testování proti Glove LSTM, Embedding Dense, Glove LSTM, Glove Dense\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "deac0217989ddcfa4345bc236e90ab22a38c5ad7d8517b867015082eaa3f672d"
  },
  "kernelspec": {
   "display_name": "Python 3.8.1 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
